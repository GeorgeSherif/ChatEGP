{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15LJzeGhGZBWboYjAF0vXgAnVMaYVClB5",
      "authorship_tag": "ABX9TyMUpo4XjikGkFkCTrMJtMUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeSherif/ChatEGP/blob/main/Translate_Ar_En.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing & Importing the Necessary Libraries and Mounting the drive**"
      ],
      "metadata": {
        "id": "OAw4nlDbEZUQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SDVJd_TDyswW"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers sentencepiece nltk protobuf torch pygal simpletransformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from IPython.display import display # Allows the use of display() for DataFrames\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UP_-BDdA37e",
        "outputId": "5dd119fb-7058-448b-e066-cdf90af04cbb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import warnings\n",
        "import csv\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pygal as py\n",
        "import matplotlib\n",
        "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
        "matplotlib.rc('xtick', labelsize=7) \n",
        "matplotlib.rc('ytick', labelsize=7) \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "%matplotlib inline "
      ],
      "metadata": {
        "id": "8eiG5KriE7bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Approach 1: Evaluate the English Model (BERT)**\n",
        "\n",
        "\n",
        "*   English Train Data\n",
        "*   Arabic Test Data\n",
        "*   Train with the English Dataset\n",
        "*   Translate the Arabic Dataset\n",
        "*   Evaluate Model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "haDPqEitD7k7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the English Dataset"
      ],
      "metadata": {
        "id": "PcdWNYwxG7b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/NLP/English Dataset.csv' ,engine=\"python\", encoding = \"ISO-8859-1\")\n",
        "df['Sentiment'] = df['Sentiment'].replace(['negative','neutral','positive'],[0,1,2])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "GHd4hXPPG6Rn",
        "outputId": "c1681599-382f-4805-9e9c-f986fa84c535"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Sentiment                                           Sentence\n",
              "0             1  According to Gran , the company has no plans t...\n",
              "1             1  Technopolis plans to develop in stages an area...\n",
              "2             0  The international electronic industry company ...\n",
              "3             2  With the new production plant the company woul...\n",
              "4             2  According to the company 's updated strategy f...\n",
              "...         ...                                                ...\n",
              "4841          0  LONDON MarketWatch -- Share prices ended lower...\n",
              "4842          1  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
              "4843          0  Operating profit fell to EUR 35.4 mn from EUR ...\n",
              "4844          0  Net sales of the Paper segment decreased to EU...\n",
              "4845          0  Sales in Finland decreased by 10.5 % in Januar...\n",
              "\n",
              "[4846 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4955e81b-6b94-46dc-948f-fd501dfc1203\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4841</th>\n",
              "      <td>0</td>\n",
              "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4842</th>\n",
              "      <td>1</td>\n",
              "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4843</th>\n",
              "      <td>0</td>\n",
              "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4844</th>\n",
              "      <td>0</td>\n",
              "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>0</td>\n",
              "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4846 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4955e81b-6b94-46dc-948f-fd501dfc1203')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4955e81b-6b94-46dc-948f-fd501dfc1203 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4955e81b-6b94-46dc-948f-fd501dfc1203');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y=\"Sentiment\",data=df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "r5E8aFGXG6Zl",
        "outputId": "f69a11d2-8210-44c4-db99-09aa680abed8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='count', ylabel='Sentiment'>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(subset=['Sentence'],keep='first',inplace=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "yIn3Q46wlVV6",
        "outputId": "7a76428f-1012-45a3-fdd3-3828d85ee39d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Sentiment                                           Sentence\n",
              "0             1  According to Gran , the company has no plans t...\n",
              "1             1  Technopolis plans to develop in stages an area...\n",
              "2             0  The international electronic industry company ...\n",
              "3             2  With the new production plant the company woul...\n",
              "4             2  According to the company 's updated strategy f...\n",
              "...         ...                                                ...\n",
              "4841          0  LONDON MarketWatch -- Share prices ended lower...\n",
              "4842          1  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
              "4843          0  Operating profit fell to EUR 35.4 mn from EUR ...\n",
              "4844          0  Net sales of the Paper segment decreased to EU...\n",
              "4845          0  Sales in Finland decreased by 10.5 % in Januar...\n",
              "\n",
              "[4838 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2cf85df-82f9-4224-8e4e-1d7bf9261bdf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4841</th>\n",
              "      <td>0</td>\n",
              "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4842</th>\n",
              "      <td>1</td>\n",
              "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4843</th>\n",
              "      <td>0</td>\n",
              "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4844</th>\n",
              "      <td>0</td>\n",
              "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>0</td>\n",
              "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4838 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2cf85df-82f9-4224-8e4e-1d7bf9261bdf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2cf85df-82f9-4224-8e4e-1d7bf9261bdf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2cf85df-82f9-4224-8e4e-1d7bf9261bdf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determinig the sentiment using TextBlob Polarity"
      ],
      "metadata": {
        "id": "WUgy1Tc6XqT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "def preprocess(ReviewText):\n",
        "    ReviewText = ReviewText.str.replace(\"(<br/>)\", \"\")\n",
        "    ReviewText = ReviewText.str.replace('(<a).*(>).*(</a>)', '')\n",
        "    ReviewText = ReviewText.str.replace('(&amp)', '')\n",
        "    ReviewText = ReviewText.str.replace('(&gt)', '')\n",
        "    ReviewText = ReviewText.str.replace('(&lt)', '')\n",
        "    ReviewText = ReviewText.str.replace('(\\xa0)', '')\n",
        "    ReviewText = ReviewText.str.replace(',', '')\n",
        "    ReviewText = ReviewText.str.replace('--', '')    \n",
        "    ReviewText = ReviewText.str.replace('`', '')    \n",
        "\n",
        "    return ReviewText\n",
        "df['Review Text'] = preprocess(df['Sentence'])\n",
        "\n",
        "df['polarity'] = df['Sentence'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
        "df['sentence_len'] = df['Review Text'].astype(str).apply(len)\n",
        "df['word_count'] = df['Sentence'].apply(lambda x: len(str(x).split()))"
      ],
      "metadata": {
        "id": "-3ssMm1PG6bz"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(df[\"sentence_len\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxTXy2-3G6i3",
        "outputId": "dc7c7ba7-c26a-4e4e-a6e1-5f4dc8f1486d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "302"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df[\"sentence_len\"] == 302]['Review Text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pQNbcsFN97Q",
        "outputId": "9919baf3-3626-4125-9fe0-7bfd506170dd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62     The new agreement is a continuation to theagr...\n",
              "Name: Review Text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"sentence_len\", \"Sentence\",\"word_count\",\"polarity\" ] , axis =1)\n"
      ],
      "metadata": {
        "id": "l6UVSjnUrJZ7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis using BERT"
      ],
      "metadata": {
        "id": "u_LkJij6lsOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_features = df[\"Review Text\"]\n",
        "Y_features = df[\"Sentiment\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(df.index.values, df.Sentiment.values, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "7fS-iD5jlygM"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYOG8axR0nev",
        "outputId": "16efde65-16b4-4a6d-ba2a-29a2c9d9bbb4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 1, ..., 1, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFqRzcKbntvM",
        "outputId": "f2ced3ef-fe6f-4483-f8ca-1cbdc6284822"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['data_type'] = ['not_set'] * df.shape[0]\n",
        "print(df[\"Review Text\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnCpXBZ60LDT",
        "outputId": "54d70f14-eb1d-45e1-d299-e61ec1757d1b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to Gran  the company has no plans to move all production to Russia  although that is where the company is growing .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[X_train]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zw_BQPXg1aGw",
        "outputId": "631de8f8-0638-48b5-97e5-f414101789a2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Sentiment                                        Review Text data_type\n",
              "4067          0  Operating profit was EUR 1.6 mn in 2005 compar...   not_set\n",
              "433           2  In September 2010  the Finnish group agreed to...   not_set\n",
              "3174          1  The businesses to be divested offer dairy  edi...   not_set\n",
              "290           2  The company is in the process of building a ne...   not_set\n",
              "2587          1  The Board of Directors was authorized to decid...   not_set\n",
              "...         ...                                                ...       ...\n",
              "4434          0  Vaisala 's net profit for the third quarter of...   not_set\n",
              "467           2   Residentialconstruction in particular has pic...   not_set\n",
              "3099          1                                       R&D Loan ) .   not_set\n",
              "3780          1  The closing of such transaction took place tod...   not_set\n",
              "862           2  ( ADP News ) - Sep 30  2008 - Finnish security...   not_set\n",
              "\n",
              "[3870 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13c249d3-94e7-4e53-a735-15a35a151438\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Review Text</th>\n",
              "      <th>data_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4067</th>\n",
              "      <td>0</td>\n",
              "      <td>Operating profit was EUR 1.6 mn in 2005 compar...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>2</td>\n",
              "      <td>In September 2010  the Finnish group agreed to...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3174</th>\n",
              "      <td>1</td>\n",
              "      <td>The businesses to be divested offer dairy  edi...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>2</td>\n",
              "      <td>The company is in the process of building a ne...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2587</th>\n",
              "      <td>1</td>\n",
              "      <td>The Board of Directors was authorized to decid...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4434</th>\n",
              "      <td>0</td>\n",
              "      <td>Vaisala 's net profit for the third quarter of...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>2</td>\n",
              "      <td>Residentialconstruction in particular has pic...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3099</th>\n",
              "      <td>1</td>\n",
              "      <td>R&amp;D Loan ) .</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3780</th>\n",
              "      <td>1</td>\n",
              "      <td>The closing of such transaction took place tod...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862</th>\n",
              "      <td>2</td>\n",
              "      <td>( ADP News ) - Sep 30  2008 - Finnish security...</td>\n",
              "      <td>not_set</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3870 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13c249d3-94e7-4e53-a735-15a35a151438')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13c249d3-94e7-4e53-a735-15a35a151438 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13c249d3-94e7-4e53-a735-15a35a151438');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[X_train, 'data_type'] = 'train'\n",
        "df.loc[X_val, 'data_type'] = 'val'\n",
        "\n",
        "#groupby count\n",
        "df.groupby([ 'Sentiment', 'data_type']).count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "XvfkG7kv0MX1",
        "outputId": "3fd78b47-7745-45d9-90b7-e3f844b84b1b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     Review Text\n",
              "Sentiment data_type             \n",
              "0         train              484\n",
              "          val                120\n",
              "1         train             2297\n",
              "          val                575\n",
              "2         train             1089\n",
              "          val                273"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d7c4813-fd86-4b8f-aa73-652565094c25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Review Text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sentiment</th>\n",
              "      <th>data_type</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
              "      <th>train</th>\n",
              "      <td>484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
              "      <th>train</th>\n",
              "      <td>2297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
              "      <th>train</th>\n",
              "      <td>1089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d7c4813-fd86-4b8f-aa73-652565094c25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d7c4813-fd86-4b8f-aa73-652565094c25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d7c4813-fd86-4b8f-aa73-652565094c25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'Review Text': 'Sentence'})\n"
      ],
      "metadata": {
        "id": "u-PACDeo5cvQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode train set\n",
        "encoded_data_train = tokenizer.batch_encode_plus(df[df.data_type == 'train'].Sentence.values,\n",
        "                                                add_special_tokens = True,\n",
        "                                                return_attention_mask = True,\n",
        "                                                pad_to_max_length = True,\n",
        "                                                max_length = 100,\n",
        "                                                return_tensors = 'pt')\n",
        "                                                \n",
        "#encode validation set\n",
        "encoded_data_val = tokenizer.batch_encode_plus( df[df.data_type == 'val'].Sentence.values,\n",
        "                                                add_special_tokens = True,\n",
        "                                                return_attention_mask = True,\n",
        "                                                pad_to_max_length = True,\n",
        "                                                max_length = 100,\n",
        "                                                return_tensors = 'pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t00AHGuvgAo",
        "outputId": "deda0f4b-cfbd-4c9b-b733-63f36358494f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "#train set\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(df[df.data_type == 'train'].Sentiment.values)\n",
        "\n",
        "#validation set\n",
        "input_ids_val = encoded_data_val['input_ids']\n",
        "attention_masks_val = encoded_data_val['attention_mask']\n",
        "labels_val = torch.tensor(df[df.data_type == 'val'].Sentiment.values)\n",
        "     "
      ],
      "metadata": {
        "id": "-J5twuCRxA7O"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah5YDfy94Jqg",
        "outputId": "a8f7b359-54ae-4bee-a842-0b66fbe0a820"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.0+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                      num_labels = 3,\n",
        "                                                      id2label={0: 'negative', 1: 'neutral', 2: 'positive'},\n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HplMCq7DxA-o",
        "outputId": "89ef649e-d2c7-4bfe-a509-ab90c32b922f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "#train set\n",
        "dataset_train = TensorDataset(input_ids_train, \n",
        "                              attention_masks_train,\n",
        "                              labels_train)\n",
        "\n",
        "#validation set\n",
        "dataset_val = TensorDataset(input_ids_val, \n",
        "                             attention_masks_val, \n",
        "                             labels_val)"
      ],
      "metadata": {
        "id": "joRW8_E34QX3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "#train set\n",
        "dataloader_train = DataLoader(dataset_train,\n",
        "                              sampler = RandomSampler(dataset_train),\n",
        "                              batch_size = batch_size)\n",
        "\n",
        "#validation set\n",
        "dataloader_val = DataLoader(dataset_val,\n",
        "                              sampler = RandomSampler(dataset_val),\n",
        "                              batch_size = 8) #since we don't have to do backpropagation for this step"
      ],
      "metadata": {
        "id": "kue1cFIC4QaZ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                 lr = 1e-5,\n",
        "                 eps = 1e-7) #2e-5 > 5e-5\n",
        "                 \n",
        "epochs = 1\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps =len(dataloader_train)*epochs)\n"
      ],
      "metadata": {
        "id": "nfrQRiNf4Qc1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader_val):\n",
        "\n",
        "    #evaluation mode \n",
        "    model.eval()\n",
        "    \n",
        "    #tracking variables\n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "    \n",
        "    for batch in tqdm(dataloader_val):\n",
        "        \n",
        "        #load into GPU\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        #define inputs\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2]}\n",
        "\n",
        "        #compute logits\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(**inputs)\n",
        "        \n",
        "        #compute loss\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        #compute accuracy\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "    \n",
        "    #compute average loss\n",
        "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "            \n",
        "    return loss_val_avg, predictions, true_vals"
      ],
      "metadata": {
        "id": "_hTFuvA04Xg_"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis = 1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, preds_flat, average = 'weighted')"
      ],
      "metadata": {
        "id": "sIzdaLGr4ara"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy score\n",
        "def accuracy_per_class(preds, labels):\n",
        "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
        "    \n",
        "    #make prediction\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    \n",
        "    for label in np.unique(labels_flat):\n",
        "        y_preds = preds_flat[labels_flat==label]\n",
        "        y_true = labels_flat[labels_flat==label]\n",
        "        #print(f'Class: {label_dict_inverse[label]}')\n",
        "        print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\\n -> {len(y_preds[y_preds==label]) / len(y_true)}')"
      ],
      "metadata": {
        "id": "Z9CCDzC14dMH"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "OehAW6vX4e7B"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    loss_train_total = 0\n",
        "    \n",
        "    progress_bar = tqdm(dataloader_train, \n",
        "                        desc = 'Epoch {:1d}'.format(epoch), \n",
        "                        leave = False, \n",
        "                        disable = False)\n",
        "    \n",
        "    for batch in progress_bar:\n",
        "        \n",
        "        model.zero_grad() #set gradient to 0\n",
        "    \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids': batch[0], \n",
        "                  'attention_mask': batch[1], \n",
        "                  'labels': batch[2]}\n",
        "        \n",
        "        outputs = model(**inputs) #unpack the dict straight into inputs\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n",
        "        \n",
        "    torch.save(model.state_dict(), f'/content/gdrive/MyDrive/NLP/BERT_ft_epoch{epoch}.model')\n",
        "    \n",
        "    tqdm.write('\\n Epoch {epoch}')\n",
        "    \n",
        "    loss_train_ave = loss_train_total / len(dataloader_train)\n",
        "    tqdm.write('Training loss: {loss_train_avg}')\n",
        "    \n",
        "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
        "    val_f1 = f1_score_func(predictions, true_vals)\n",
        "    tqdm.write(f'Validation loss: {val_loss}')\n",
        "    tqdm.write(f'F1 Score (weighted): {val_f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d5EMKCDo4hmk",
        "outputId": "d958e99b-7939-4725-fdac-321169b44158"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Epoch 1:   0%|          | 0/484 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:   0%|          | 0/484 [00:19<?, ?it/s, training_loss=0.327]\u001b[A\n",
            "Epoch 1:   0%|          | 1/484 [00:19<2:40:10, 19.90s/it, training_loss=0.327]\u001b[A\n",
            "Epoch 1:   0%|          | 1/484 [00:36<2:40:10, 19.90s/it, training_loss=0.360]\u001b[A\n",
            "Epoch 1:   0%|          | 2/484 [00:36<2:23:27, 17.86s/it, training_loss=0.360]\u001b[A\n",
            "Epoch 1:   0%|          | 2/484 [00:50<2:23:27, 17.86s/it, training_loss=0.365]\u001b[A\n",
            "Epoch 1:   1%|          | 3/484 [00:50<2:08:09, 15.99s/it, training_loss=0.365]\u001b[A\n",
            "Epoch 1:   1%|          | 3/484 [00:59<2:08:09, 15.99s/it, training_loss=0.381]\u001b[A\n",
            "Epoch 1:   1%|          | 4/484 [00:59<1:46:13, 13.28s/it, training_loss=0.381]\u001b[A\n",
            "Epoch 1:   1%|          | 4/484 [01:10<1:46:13, 13.28s/it, training_loss=0.349]\u001b[A\n",
            "Epoch 1:   1%|          | 5/484 [01:10<1:40:52, 12.63s/it, training_loss=0.349]\u001b[A\n",
            "Epoch 1:   1%|          | 5/484 [01:18<1:40:52, 12.63s/it, training_loss=0.295]\u001b[A\n",
            "Epoch 1:   1%|          | 6/484 [01:18<1:28:48, 11.15s/it, training_loss=0.295]\u001b[A\n",
            "Epoch 1:   1%|          | 6/484 [01:30<1:28:48, 11.15s/it, training_loss=0.320]\u001b[A\n",
            "Epoch 1:   1%|▏         | 7/484 [01:30<1:28:28, 11.13s/it, training_loss=0.320]\u001b[A\n",
            "Epoch 1:   1%|▏         | 7/484 [01:38<1:28:28, 11.13s/it, training_loss=0.341]\u001b[A\n",
            "Epoch 1:   2%|▏         | 8/484 [01:38<1:21:43, 10.30s/it, training_loss=0.341]\u001b[A\n",
            "Epoch 1:   2%|▏         | 8/484 [01:50<1:21:43, 10.30s/it, training_loss=0.370]\u001b[A\n",
            "Epoch 1:   2%|▏         | 9/484 [01:50<1:25:52, 10.85s/it, training_loss=0.370]\u001b[A\n",
            "Epoch 1:   2%|▏         | 9/484 [01:59<1:25:52, 10.85s/it, training_loss=0.314]\u001b[A\n",
            "Epoch 1:   2%|▏         | 10/484 [01:59<1:20:19, 10.17s/it, training_loss=0.314]\u001b[A\n",
            "Epoch 1:   2%|▏         | 10/484 [02:09<1:20:19, 10.17s/it, training_loss=0.304]\u001b[A\n",
            "Epoch 1:   2%|▏         | 11/484 [02:09<1:20:51, 10.26s/it, training_loss=0.304]\u001b[A\n",
            "Epoch 1:   2%|▏         | 11/484 [02:18<1:20:51, 10.26s/it, training_loss=0.390]\u001b[A\n",
            "Epoch 1:   2%|▏         | 12/484 [02:18<1:17:26,  9.84s/it, training_loss=0.390]\u001b[A\n",
            "Epoch 1:   2%|▏         | 12/484 [02:28<1:17:26,  9.84s/it, training_loss=0.397]\u001b[A\n",
            "Epoch 1:   3%|▎         | 13/484 [02:28<1:17:29,  9.87s/it, training_loss=0.397]\u001b[A\n",
            "Epoch 1:   3%|▎         | 13/484 [02:40<1:17:29,  9.87s/it, training_loss=0.343]\u001b[A\n",
            "Epoch 1:   3%|▎         | 14/484 [02:40<1:21:47, 10.44s/it, training_loss=0.343]\u001b[A\n",
            "Epoch 1:   3%|▎         | 14/484 [02:50<1:21:47, 10.44s/it, training_loss=0.277]\u001b[A\n",
            "Epoch 1:   3%|▎         | 15/484 [02:50<1:22:00, 10.49s/it, training_loss=0.277]\u001b[A\n",
            "Epoch 1:   3%|▎         | 15/484 [03:01<1:22:00, 10.49s/it, training_loss=0.405]\u001b[A\n",
            "Epoch 1:   3%|▎         | 16/484 [03:01<1:23:03, 10.65s/it, training_loss=0.405]\u001b[A\n",
            "Epoch 1:   3%|▎         | 16/484 [03:10<1:23:03, 10.65s/it, training_loss=0.341]\u001b[A\n",
            "Epoch 1:   4%|▎         | 17/484 [03:10<1:17:49, 10.00s/it, training_loss=0.341]\u001b[A\n",
            "Epoch 1:   4%|▎         | 17/484 [03:21<1:17:49, 10.00s/it, training_loss=0.319]\u001b[A\n",
            "Epoch 1:   4%|▎         | 18/484 [03:21<1:20:36, 10.38s/it, training_loss=0.319]\u001b[A\n",
            "Epoch 1:   4%|▎         | 18/484 [03:29<1:20:36, 10.38s/it, training_loss=0.288]\u001b[A\n",
            "Epoch 1:   4%|▍         | 19/484 [03:29<1:15:17,  9.71s/it, training_loss=0.288]\u001b[A\n",
            "Epoch 1:   4%|▍         | 19/484 [03:41<1:15:17,  9.71s/it, training_loss=0.296]\u001b[A\n",
            "Epoch 1:   4%|▍         | 20/484 [03:41<1:19:12, 10.24s/it, training_loss=0.296]\u001b[A\n",
            "Epoch 1:   4%|▍         | 20/484 [03:49<1:19:12, 10.24s/it, training_loss=0.327]\u001b[A\n",
            "Epoch 1:   4%|▍         | 21/484 [03:49<1:13:53,  9.58s/it, training_loss=0.327]\u001b[A\n",
            "Epoch 1:   4%|▍         | 21/484 [04:01<1:13:53,  9.58s/it, training_loss=0.338]\u001b[A\n",
            "Epoch 1:   5%|▍         | 22/484 [04:01<1:18:56, 10.25s/it, training_loss=0.338]\u001b[A\n",
            "Epoch 1:   5%|▍         | 22/484 [04:09<1:18:56, 10.25s/it, training_loss=0.356]\u001b[A\n",
            "Epoch 1:   5%|▍         | 23/484 [04:09<1:14:06,  9.64s/it, training_loss=0.356]\u001b[A\n",
            "Epoch 1:   5%|▍         | 23/484 [04:20<1:14:06,  9.64s/it, training_loss=0.322]\u001b[A\n",
            "Epoch 1:   5%|▍         | 24/484 [04:20<1:17:18, 10.08s/it, training_loss=0.322]\u001b[A\n",
            "Epoch 1:   5%|▍         | 24/484 [04:28<1:17:18, 10.08s/it, training_loss=0.385]\u001b[A\n",
            "Epoch 1:   5%|▌         | 25/484 [04:28<1:12:51,  9.52s/it, training_loss=0.385]\u001b[A\n",
            "Epoch 1:   5%|▌         | 25/484 [04:40<1:12:51,  9.52s/it, training_loss=0.293]\u001b[A\n",
            "Epoch 1:   5%|▌         | 26/484 [04:40<1:17:16, 10.12s/it, training_loss=0.293]\u001b[A\n",
            "Epoch 1:   5%|▌         | 26/484 [04:53<1:17:16, 10.12s/it, training_loss=0.267]\u001b[A\n",
            "Epoch 1:   6%|▌         | 27/484 [04:53<1:23:18, 10.94s/it, training_loss=0.267]\u001b[A\n",
            "Epoch 1:   6%|▌         | 27/484 [05:02<1:23:18, 10.94s/it, training_loss=0.345]\u001b[A\n",
            "Epoch 1:   6%|▌         | 28/484 [05:02<1:18:42, 10.36s/it, training_loss=0.345]\u001b[A\n",
            "Epoch 1:   6%|▌         | 28/484 [05:12<1:18:42, 10.36s/it, training_loss=0.248]\u001b[A\n",
            "Epoch 1:   6%|▌         | 29/484 [05:12<1:17:54, 10.27s/it, training_loss=0.248]\u001b[A\n",
            "Epoch 1:   6%|▌         | 29/484 [05:21<1:17:54, 10.27s/it, training_loss=0.304]\u001b[A\n",
            "Epoch 1:   6%|▌         | 30/484 [05:21<1:15:26,  9.97s/it, training_loss=0.304]\u001b[A\n",
            "Epoch 1:   6%|▌         | 30/484 [05:31<1:15:26,  9.97s/it, training_loss=0.317]\u001b[A\n",
            "Epoch 1:   6%|▋         | 31/484 [05:31<1:16:14, 10.10s/it, training_loss=0.317]\u001b[A\n",
            "Epoch 1:   6%|▋         | 31/484 [05:40<1:16:14, 10.10s/it, training_loss=0.368]\u001b[A\n",
            "Epoch 1:   7%|▋         | 32/484 [05:40<1:13:42,  9.78s/it, training_loss=0.368]\u001b[A\n",
            "Epoch 1:   7%|▋         | 32/484 [05:51<1:13:42,  9.78s/it, training_loss=0.328]\u001b[A\n",
            "Epoch 1:   7%|▋         | 33/484 [05:51<1:16:18, 10.15s/it, training_loss=0.328]\u001b[A\n",
            "Epoch 1:   7%|▋         | 33/484 [06:00<1:16:18, 10.15s/it, training_loss=0.300]\u001b[A\n",
            "Epoch 1:   7%|▋         | 34/484 [06:00<1:12:43,  9.70s/it, training_loss=0.300]\u001b[A\n",
            "Epoch 1:   7%|▋         | 34/484 [06:11<1:12:43,  9.70s/it, training_loss=0.378]\u001b[A\n",
            "Epoch 1:   7%|▋         | 35/484 [06:11<1:15:58, 10.15s/it, training_loss=0.378]\u001b[A\n",
            "Epoch 1:   7%|▋         | 35/484 [06:19<1:15:58, 10.15s/it, training_loss=0.218]\u001b[A\n",
            "Epoch 1:   7%|▋         | 36/484 [06:19<1:10:55,  9.50s/it, training_loss=0.218]\u001b[A\n",
            "Epoch 1:   7%|▋         | 36/484 [06:31<1:10:55,  9.50s/it, training_loss=0.363]\u001b[A\n",
            "Epoch 1:   8%|▊         | 37/484 [06:31<1:15:32, 10.14s/it, training_loss=0.363]\u001b[A\n",
            "Epoch 1:   8%|▊         | 37/484 [06:39<1:15:32, 10.14s/it, training_loss=0.282]\u001b[A\n",
            "Epoch 1:   8%|▊         | 38/484 [06:39<1:10:36,  9.50s/it, training_loss=0.282]\u001b[A\n",
            "Epoch 1:   8%|▊         | 38/484 [06:50<1:10:36,  9.50s/it, training_loss=0.310]\u001b[A\n",
            "Epoch 1:   8%|▊         | 39/484 [06:50<1:14:14, 10.01s/it, training_loss=0.310]\u001b[A\n",
            "Epoch 1:   8%|▊         | 39/484 [06:58<1:14:14, 10.01s/it, training_loss=0.267]\u001b[A\n",
            "Epoch 1:   8%|▊         | 40/484 [06:58<1:09:54,  9.45s/it, training_loss=0.267]\u001b[A\n",
            "Epoch 1:   8%|▊         | 40/484 [07:10<1:09:54,  9.45s/it, training_loss=0.421]\u001b[A\n",
            "Epoch 1:   8%|▊         | 41/484 [07:10<1:14:18, 10.06s/it, training_loss=0.421]\u001b[A\n",
            "Epoch 1:   8%|▊         | 41/484 [07:18<1:14:18, 10.06s/it, training_loss=0.392]\u001b[A\n",
            "Epoch 1:   9%|▊         | 42/484 [07:18<1:09:50,  9.48s/it, training_loss=0.392]\u001b[A\n",
            "Epoch 1:   9%|▊         | 42/484 [07:29<1:09:50,  9.48s/it, training_loss=0.220]\u001b[A\n",
            "Epoch 1:   9%|▉         | 43/484 [07:29<1:12:54,  9.92s/it, training_loss=0.220]\u001b[A\n",
            "Epoch 1:   9%|▉         | 43/484 [07:37<1:12:54,  9.92s/it, training_loss=0.251]\u001b[A\n",
            "Epoch 1:   9%|▉         | 44/484 [07:37<1:09:02,  9.42s/it, training_loss=0.251]\u001b[A\n",
            "Epoch 1:   9%|▉         | 44/484 [07:48<1:09:02,  9.42s/it, training_loss=0.374]\u001b[A\n",
            "Epoch 1:   9%|▉         | 45/484 [07:48<1:13:11, 10.00s/it, training_loss=0.374]\u001b[A\n",
            "Epoch 1:   9%|▉         | 45/484 [07:57<1:13:11, 10.00s/it, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  10%|▉         | 46/484 [07:57<1:09:55,  9.58s/it, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  10%|▉         | 46/484 [08:07<1:09:55,  9.58s/it, training_loss=0.288]\u001b[A\n",
            "Epoch 1:  10%|▉         | 47/484 [08:07<1:11:24,  9.81s/it, training_loss=0.288]\u001b[A\n",
            "Epoch 1:  10%|▉         | 47/484 [08:17<1:11:24,  9.81s/it, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  10%|▉         | 48/484 [08:17<1:09:56,  9.62s/it, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  10%|▉         | 48/484 [08:27<1:09:56,  9.62s/it, training_loss=0.340]\u001b[A\n",
            "Epoch 1:  10%|█         | 49/484 [08:27<1:11:26,  9.85s/it, training_loss=0.340]\u001b[A\n",
            "Epoch 1:  10%|█         | 49/484 [08:36<1:11:26,  9.85s/it, training_loss=0.279]\u001b[A\n",
            "Epoch 1:  10%|█         | 50/484 [08:36<1:10:04,  9.69s/it, training_loss=0.279]\u001b[A\n",
            "Epoch 1:  10%|█         | 50/484 [08:50<1:10:04,  9.69s/it, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  11%|█         | 51/484 [08:50<1:18:10, 10.83s/it, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  11%|█         | 51/484 [08:59<1:18:10, 10.83s/it, training_loss=0.308]\u001b[A\n",
            "Epoch 1:  11%|█         | 52/484 [08:59<1:15:27, 10.48s/it, training_loss=0.308]\u001b[A\n",
            "Epoch 1:  11%|█         | 52/484 [09:09<1:15:27, 10.48s/it, training_loss=0.281]\u001b[A\n",
            "Epoch 1:  11%|█         | 53/484 [09:09<1:13:16, 10.20s/it, training_loss=0.281]\u001b[A\n",
            "Epoch 1:  11%|█         | 53/484 [09:18<1:13:16, 10.20s/it, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  11%|█         | 54/484 [09:18<1:11:22,  9.96s/it, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  11%|█         | 54/484 [09:27<1:11:22,  9.96s/it, training_loss=0.250]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 55/484 [09:27<1:09:31,  9.72s/it, training_loss=0.250]\u001b[A\n",
            "Epoch 1:  11%|█▏        | 55/484 [09:37<1:09:31,  9.72s/it, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 56/484 [09:37<1:09:12,  9.70s/it, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 56/484 [09:46<1:09:12,  9.70s/it, training_loss=0.374]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 57/484 [09:46<1:07:36,  9.50s/it, training_loss=0.374]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 57/484 [09:56<1:07:36,  9.50s/it, training_loss=0.347]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 58/484 [09:56<1:08:55,  9.71s/it, training_loss=0.347]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 58/484 [10:05<1:08:55,  9.71s/it, training_loss=0.304]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 59/484 [10:05<1:06:21,  9.37s/it, training_loss=0.304]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 59/484 [10:15<1:06:21,  9.37s/it, training_loss=0.337]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 60/484 [10:15<1:08:20,  9.67s/it, training_loss=0.337]\u001b[A\n",
            "Epoch 1:  12%|█▏        | 60/484 [10:24<1:08:20,  9.67s/it, training_loss=0.324]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 61/484 [10:24<1:05:22,  9.27s/it, training_loss=0.324]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 61/484 [10:34<1:05:22,  9.27s/it, training_loss=0.253]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 62/484 [10:34<1:07:52,  9.65s/it, training_loss=0.253]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 62/484 [10:49<1:07:52,  9.65s/it, training_loss=0.324]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 63/484 [10:49<1:19:11, 11.29s/it, training_loss=0.324]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 63/484 [11:04<1:19:11, 11.29s/it, training_loss=0.233]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 64/484 [11:04<1:26:09, 12.31s/it, training_loss=0.233]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 64/484 [11:22<1:26:09, 12.31s/it, training_loss=0.408]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 65/484 [11:22<1:37:28, 13.96s/it, training_loss=0.408]\u001b[A\n",
            "Epoch 1:  13%|█▎        | 65/484 [11:33<1:37:28, 13.96s/it, training_loss=0.276]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 66/484 [11:33<1:31:54, 13.19s/it, training_loss=0.276]\u001b[A\n",
            "Epoch 1:  14%|█▎        | 66/484 [11:42<1:31:54, 13.19s/it, training_loss=0.336]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 67/484 [11:42<1:23:07, 11.96s/it, training_loss=0.336]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 67/484 [11:53<1:23:07, 11.96s/it, training_loss=0.293]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 68/484 [11:53<1:20:45, 11.65s/it, training_loss=0.293]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 68/484 [12:01<1:20:45, 11.65s/it, training_loss=0.251]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 69/484 [12:01<1:13:28, 10.62s/it, training_loss=0.251]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 69/484 [12:12<1:13:28, 10.62s/it, training_loss=0.271]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 70/484 [12:12<1:12:42, 10.54s/it, training_loss=0.271]\u001b[A\n",
            "Epoch 1:  14%|█▍        | 70/484 [12:20<1:12:42, 10.54s/it, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 71/484 [12:20<1:08:17,  9.92s/it, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 71/484 [12:30<1:08:17,  9.92s/it, training_loss=0.231]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 72/484 [12:30<1:08:17,  9.95s/it, training_loss=0.231]\u001b[A\n",
            "Epoch 1:  15%|█▍        | 72/484 [12:39<1:08:17,  9.95s/it, training_loss=0.275]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 73/484 [12:39<1:06:30,  9.71s/it, training_loss=0.275]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 73/484 [12:53<1:06:30,  9.71s/it, training_loss=0.238]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 74/484 [12:53<1:13:56, 10.82s/it, training_loss=0.238]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 74/484 [13:02<1:13:56, 10.82s/it, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 75/484 [13:02<1:09:43, 10.23s/it, training_loss=0.261]\u001b[A\n",
            "Epoch 1:  15%|█▌        | 75/484 [13:12<1:09:43, 10.23s/it, training_loss=0.198]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 76/484 [13:12<1:10:42, 10.40s/it, training_loss=0.198]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 76/484 [13:21<1:10:42, 10.40s/it, training_loss=0.245]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 77/484 [13:21<1:05:45,  9.69s/it, training_loss=0.245]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 77/484 [13:31<1:05:45,  9.69s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 78/484 [13:31<1:07:05,  9.92s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  16%|█▌        | 78/484 [13:42<1:07:05,  9.92s/it, training_loss=0.182]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 79/484 [13:42<1:08:33, 10.16s/it, training_loss=0.182]\u001b[A\n",
            "Epoch 1:  16%|█▋        | 79/484 [13:51<1:08:33, 10.16s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 80/484 [13:51<1:06:40,  9.90s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 80/484 [14:01<1:06:40,  9.90s/it, training_loss=0.279]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 81/484 [14:01<1:05:51,  9.81s/it, training_loss=0.279]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 81/484 [14:10<1:05:51,  9.81s/it, training_loss=0.239]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 82/484 [14:10<1:04:13,  9.59s/it, training_loss=0.239]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 82/484 [14:20<1:04:13,  9.59s/it, training_loss=0.167]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 83/484 [14:20<1:04:48,  9.70s/it, training_loss=0.167]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 83/484 [14:28<1:04:48,  9.70s/it, training_loss=0.299]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 84/484 [14:28<1:02:50,  9.43s/it, training_loss=0.299]\u001b[A\n",
            "Epoch 1:  17%|█▋        | 84/484 [14:39<1:02:50,  9.43s/it, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 85/484 [14:39<1:04:06,  9.64s/it, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 85/484 [14:47<1:04:06,  9.64s/it, training_loss=0.209]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 86/484 [14:47<1:01:44,  9.31s/it, training_loss=0.209]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 86/484 [14:58<1:01:44,  9.31s/it, training_loss=0.153]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 87/484 [14:58<1:05:36,  9.91s/it, training_loss=0.153]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 87/484 [15:07<1:05:36,  9.91s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 88/484 [15:07<1:02:28,  9.47s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 88/484 [15:17<1:02:28,  9.47s/it, training_loss=0.184]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 89/484 [15:17<1:04:29,  9.80s/it, training_loss=0.184]\u001b[A\n",
            "Epoch 1:  18%|█▊        | 89/484 [15:26<1:04:29,  9.80s/it, training_loss=0.225]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 90/484 [15:26<1:01:11,  9.32s/it, training_loss=0.225]\u001b[A\n",
            "Epoch 1:  19%|█▊        | 90/484 [15:36<1:01:11,  9.32s/it, training_loss=0.106]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 91/484 [15:36<1:04:00,  9.77s/it, training_loss=0.106]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 91/484 [15:44<1:04:00,  9.77s/it, training_loss=0.341]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 92/484 [15:44<1:00:23,  9.24s/it, training_loss=0.341]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 92/484 [15:55<1:00:23,  9.24s/it, training_loss=0.398]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 93/484 [15:55<1:03:43,  9.78s/it, training_loss=0.398]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 93/484 [16:03<1:03:43,  9.78s/it, training_loss=0.218]\u001b[A\n",
            "Epoch 1:  19%|█▉        | 94/484 [16:03<59:48,  9.20s/it, training_loss=0.218]  \u001b[A\n",
            "Epoch 1:  19%|█▉        | 94/484 [16:14<59:48,  9.20s/it, training_loss=0.371]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 95/484 [16:14<1:03:08,  9.74s/it, training_loss=0.371]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 95/484 [16:22<1:03:08,  9.74s/it, training_loss=0.277]\u001b[A\n",
            "Epoch 1:  20%|█▉        | 96/484 [16:22<59:23,  9.19s/it, training_loss=0.277]  \u001b[A\n",
            "Epoch 1:  20%|█▉        | 96/484 [16:33<59:23,  9.19s/it, training_loss=0.202]\u001b[A\n",
            "Epoch 1:  20%|██        | 97/484 [16:33<1:02:36,  9.71s/it, training_loss=0.202]\u001b[A\n",
            "Epoch 1:  20%|██        | 97/484 [16:45<1:02:36,  9.71s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  20%|██        | 98/484 [16:45<1:06:37, 10.36s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  20%|██        | 98/484 [16:55<1:06:37, 10.36s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  20%|██        | 99/484 [16:55<1:05:34, 10.22s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  20%|██        | 99/484 [17:04<1:05:34, 10.22s/it, training_loss=0.243]\u001b[A\n",
            "Epoch 1:  21%|██        | 100/484 [17:04<1:02:47,  9.81s/it, training_loss=0.243]\u001b[A\n",
            "Epoch 1:  21%|██        | 100/484 [17:13<1:02:47,  9.81s/it, training_loss=0.228]\u001b[A\n",
            "Epoch 1:  21%|██        | 101/484 [17:13<1:02:27,  9.79s/it, training_loss=0.228]\u001b[A\n",
            "Epoch 1:  21%|██        | 101/484 [17:22<1:02:27,  9.79s/it, training_loss=0.390]\u001b[A\n",
            "Epoch 1:  21%|██        | 102/484 [17:22<1:00:46,  9.55s/it, training_loss=0.390]\u001b[A\n",
            "Epoch 1:  21%|██        | 102/484 [17:32<1:00:46,  9.55s/it, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 103/484 [17:32<1:00:38,  9.55s/it, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 103/484 [17:41<1:00:38,  9.55s/it, training_loss=0.157]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 104/484 [17:41<1:00:14,  9.51s/it, training_loss=0.157]\u001b[A\n",
            "Epoch 1:  21%|██▏       | 104/484 [17:51<1:00:14,  9.51s/it, training_loss=0.268]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 105/484 [17:51<59:37,  9.44s/it, training_loss=0.268]  \u001b[A\n",
            "Epoch 1:  22%|██▏       | 105/484 [18:01<59:37,  9.44s/it, training_loss=0.289]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 106/484 [18:01<1:00:09,  9.55s/it, training_loss=0.289]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 106/484 [18:09<1:00:09,  9.55s/it, training_loss=0.310]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 107/484 [18:09<58:43,  9.35s/it, training_loss=0.310]  \u001b[A\n",
            "Epoch 1:  22%|██▏       | 107/484 [18:19<58:43,  9.35s/it, training_loss=0.237]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 108/484 [18:19<59:41,  9.52s/it, training_loss=0.237]\u001b[A\n",
            "Epoch 1:  22%|██▏       | 108/484 [18:28<59:41,  9.52s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 109/484 [18:28<58:01,  9.28s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 109/484 [18:38<58:01,  9.28s/it, training_loss=0.238]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 110/484 [18:38<59:19,  9.52s/it, training_loss=0.238]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 110/484 [18:47<59:19,  9.52s/it, training_loss=0.171]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 111/484 [18:47<57:25,  9.24s/it, training_loss=0.171]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 111/484 [18:57<57:25,  9.24s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 112/484 [18:57<59:16,  9.56s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 112/484 [19:05<59:16,  9.56s/it, training_loss=0.353]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 113/484 [19:05<56:58,  9.21s/it, training_loss=0.353]\u001b[A\n",
            "Epoch 1:  23%|██▎       | 113/484 [19:16<56:58,  9.21s/it, training_loss=0.247]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 114/484 [19:16<59:24,  9.63s/it, training_loss=0.247]\u001b[A\n",
            "Epoch 1:  24%|██▎       | 114/484 [19:24<59:24,  9.63s/it, training_loss=0.384]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 115/484 [19:24<56:32,  9.19s/it, training_loss=0.384]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 115/484 [19:35<56:32,  9.19s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 116/484 [19:35<59:18,  9.67s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 116/484 [19:43<59:18,  9.67s/it, training_loss=0.299]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 117/484 [19:43<56:05,  9.17s/it, training_loss=0.299]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 117/484 [19:54<56:05,  9.17s/it, training_loss=0.147]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 118/484 [19:54<59:15,  9.71s/it, training_loss=0.147]\u001b[A\n",
            "Epoch 1:  24%|██▍       | 118/484 [20:02<59:15,  9.71s/it, training_loss=0.333]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 119/484 [20:02<55:44,  9.16s/it, training_loss=0.333]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 119/484 [20:13<55:44,  9.16s/it, training_loss=0.194]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 120/484 [20:13<58:52,  9.70s/it, training_loss=0.194]\u001b[A\n",
            "Epoch 1:  25%|██▍       | 120/484 [20:21<58:52,  9.70s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 121/484 [20:21<55:09,  9.12s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 121/484 [20:32<55:09,  9.12s/it, training_loss=0.230]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 122/484 [20:32<58:18,  9.67s/it, training_loss=0.230]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 122/484 [20:43<58:18,  9.67s/it, training_loss=0.328]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 123/484 [20:43<1:00:47, 10.10s/it, training_loss=0.328]\u001b[A\n",
            "Epoch 1:  25%|██▌       | 123/484 [20:53<1:00:47, 10.10s/it, training_loss=0.189]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 124/484 [20:53<1:01:39, 10.28s/it, training_loss=0.189]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 124/484 [21:02<1:01:39, 10.28s/it, training_loss=0.249]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 125/484 [21:02<58:54,  9.85s/it, training_loss=0.249]  \u001b[A\n",
            "Epoch 1:  26%|██▌       | 125/484 [21:12<58:54,  9.85s/it, training_loss=0.240]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 126/484 [21:12<58:57,  9.88s/it, training_loss=0.240]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 126/484 [21:21<58:57,  9.88s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 127/484 [21:21<56:41,  9.53s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  26%|██▌       | 127/484 [21:31<56:41,  9.53s/it, training_loss=0.188]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 128/484 [21:31<56:52,  9.59s/it, training_loss=0.188]\u001b[A\n",
            "Epoch 1:  26%|██▋       | 128/484 [21:40<56:52,  9.59s/it, training_loss=0.239]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 129/484 [21:40<55:43,  9.42s/it, training_loss=0.239]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 129/484 [21:49<55:43,  9.42s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 130/484 [21:49<55:53,  9.47s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 130/484 [21:59<55:53,  9.47s/it, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 131/484 [21:59<55:36,  9.45s/it, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 131/484 [22:08<55:36,  9.45s/it, training_loss=0.207]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 132/484 [22:08<55:42,  9.49s/it, training_loss=0.207]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 132/484 [22:18<55:42,  9.49s/it, training_loss=0.214]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 133/484 [22:18<56:19,  9.63s/it, training_loss=0.214]\u001b[A\n",
            "Epoch 1:  27%|██▋       | 133/484 [22:27<56:19,  9.63s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 134/484 [22:27<54:34,  9.36s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 134/484 [22:37<54:34,  9.36s/it, training_loss=0.216]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 135/484 [22:37<55:40,  9.57s/it, training_loss=0.216]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 135/484 [22:46<55:40,  9.57s/it, training_loss=0.257]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 136/484 [22:46<53:55,  9.30s/it, training_loss=0.257]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 136/484 [22:56<53:55,  9.30s/it, training_loss=0.229]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 137/484 [22:56<55:28,  9.59s/it, training_loss=0.229]\u001b[A\n",
            "Epoch 1:  28%|██▊       | 137/484 [23:04<55:28,  9.59s/it, training_loss=0.189]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 138/484 [23:04<53:18,  9.24s/it, training_loss=0.189]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 138/484 [23:15<53:18,  9.24s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 139/484 [23:15<55:12,  9.60s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  29%|██▊       | 139/484 [23:23<55:12,  9.60s/it, training_loss=0.349]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 140/484 [23:23<52:40,  9.19s/it, training_loss=0.349]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 140/484 [23:34<52:40,  9.19s/it, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 141/484 [23:34<55:06,  9.64s/it, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 141/484 [23:42<55:06,  9.64s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 142/484 [23:42<52:15,  9.17s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  29%|██▉       | 142/484 [23:53<52:15,  9.17s/it, training_loss=0.303]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 143/484 [23:53<55:09,  9.70s/it, training_loss=0.303]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 143/484 [24:01<55:09,  9.70s/it, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 144/484 [24:01<51:53,  9.16s/it, training_loss=0.280]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 144/484 [24:12<51:53,  9.16s/it, training_loss=0.152]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 145/484 [24:12<54:51,  9.71s/it, training_loss=0.152]\u001b[A\n",
            "Epoch 1:  30%|██▉       | 145/484 [24:19<54:51,  9.71s/it, training_loss=0.164]\u001b[A\n",
            "Epoch 1:  30%|███       | 146/484 [24:19<51:37,  9.16s/it, training_loss=0.164]\u001b[A\n",
            "Epoch 1:  30%|███       | 146/484 [24:30<51:37,  9.16s/it, training_loss=0.266]\u001b[A\n",
            "Epoch 1:  30%|███       | 147/484 [24:30<54:30,  9.70s/it, training_loss=0.266]\u001b[A\n",
            "Epoch 1:  30%|███       | 147/484 [24:40<54:30,  9.70s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  31%|███       | 148/484 [24:40<54:44,  9.78s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  31%|███       | 148/484 [24:52<54:44,  9.78s/it, training_loss=0.159]\u001b[A\n",
            "Epoch 1:  31%|███       | 149/484 [24:52<58:32, 10.48s/it, training_loss=0.159]\u001b[A\n",
            "Epoch 1:  31%|███       | 149/484 [25:00<58:32, 10.48s/it, training_loss=0.446]\u001b[A\n",
            "Epoch 1:  31%|███       | 150/484 [25:00<53:57,  9.69s/it, training_loss=0.446]\u001b[A\n",
            "Epoch 1:  31%|███       | 150/484 [25:11<53:57,  9.69s/it, training_loss=0.150]\u001b[A\n",
            "Epoch 1:  31%|███       | 151/484 [25:11<55:55, 10.08s/it, training_loss=0.150]\u001b[A\n",
            "Epoch 1:  31%|███       | 151/484 [25:19<55:55, 10.08s/it, training_loss=0.192]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 152/484 [25:19<52:18,  9.45s/it, training_loss=0.192]\u001b[A\n",
            "Epoch 1:  31%|███▏      | 152/484 [25:30<52:18,  9.45s/it, training_loss=0.253]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 153/484 [25:30<54:26,  9.87s/it, training_loss=0.253]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 153/484 [25:38<54:26,  9.87s/it, training_loss=0.245]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 154/484 [25:38<51:22,  9.34s/it, training_loss=0.245]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 154/484 [25:49<51:22,  9.34s/it, training_loss=0.224]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 155/484 [25:49<53:26,  9.75s/it, training_loss=0.224]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 155/484 [25:57<53:26,  9.75s/it, training_loss=0.122]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 156/484 [25:57<51:11,  9.36s/it, training_loss=0.122]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 156/484 [26:08<51:11,  9.36s/it, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 157/484 [26:08<52:20,  9.60s/it, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  32%|███▏      | 157/484 [26:16<52:20,  9.60s/it, training_loss=0.226]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 158/484 [26:16<50:57,  9.38s/it, training_loss=0.226]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 158/484 [26:26<50:57,  9.38s/it, training_loss=0.268]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 159/484 [26:26<51:28,  9.50s/it, training_loss=0.268]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 159/484 [26:35<51:28,  9.50s/it, training_loss=0.133]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 160/484 [26:35<50:45,  9.40s/it, training_loss=0.133]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 160/484 [26:45<50:45,  9.40s/it, training_loss=0.329]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 161/484 [26:45<50:41,  9.42s/it, training_loss=0.329]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 161/484 [26:54<50:41,  9.42s/it, training_loss=0.255]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 162/484 [26:54<50:32,  9.42s/it, training_loss=0.255]\u001b[A\n",
            "Epoch 1:  33%|███▎      | 162/484 [27:03<50:32,  9.42s/it, training_loss=0.265]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 163/484 [27:03<49:55,  9.33s/it, training_loss=0.265]\u001b[A\n",
            "Epoch 1:  34%|███▎      | 163/484 [27:13<49:55,  9.33s/it, training_loss=0.190]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 164/484 [27:13<50:24,  9.45s/it, training_loss=0.190]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 164/484 [27:22<50:24,  9.45s/it, training_loss=0.176]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 165/484 [27:22<49:32,  9.32s/it, training_loss=0.176]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 165/484 [27:32<49:32,  9.32s/it, training_loss=0.200]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 166/484 [27:32<50:32,  9.54s/it, training_loss=0.200]\u001b[A\n",
            "Epoch 1:  34%|███▍      | 166/484 [27:41<50:32,  9.54s/it, training_loss=0.259]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 167/484 [27:41<49:12,  9.31s/it, training_loss=0.259]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 167/484 [27:51<49:12,  9.31s/it, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 168/484 [27:51<50:44,  9.63s/it, training_loss=0.327]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 168/484 [28:00<50:44,  9.63s/it, training_loss=0.212]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 169/484 [28:00<48:40,  9.27s/it, training_loss=0.212]\u001b[A\n",
            "Epoch 1:  35%|███▍      | 169/484 [28:10<48:40,  9.27s/it, training_loss=0.184]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 170/484 [28:10<50:20,  9.62s/it, training_loss=0.184]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 170/484 [28:18<50:20,  9.62s/it, training_loss=0.256]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 171/484 [28:18<48:02,  9.21s/it, training_loss=0.256]\u001b[A\n",
            "Epoch 1:  35%|███▌      | 171/484 [28:29<48:02,  9.21s/it, training_loss=0.218]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 172/484 [28:29<50:09,  9.65s/it, training_loss=0.218]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 172/484 [28:37<50:09,  9.65s/it, training_loss=0.204]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 173/484 [28:37<47:47,  9.22s/it, training_loss=0.204]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 173/484 [28:51<47:47,  9.22s/it, training_loss=0.342]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 174/484 [28:51<54:12, 10.49s/it, training_loss=0.342]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 174/484 [28:59<54:12, 10.49s/it, training_loss=0.234]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 175/484 [28:59<50:33,  9.82s/it, training_loss=0.234]\u001b[A\n",
            "Epoch 1:  36%|███▌      | 175/484 [29:10<50:33,  9.82s/it, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 176/484 [29:10<51:50, 10.10s/it, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  36%|███▋      | 176/484 [29:18<51:50, 10.10s/it, training_loss=0.252]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 177/484 [29:18<48:31,  9.48s/it, training_loss=0.252]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 177/484 [29:29<48:31,  9.48s/it, training_loss=0.220]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 178/484 [29:29<50:31,  9.91s/it, training_loss=0.220]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 178/484 [29:37<50:31,  9.91s/it, training_loss=0.143]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 179/484 [29:37<47:08,  9.27s/it, training_loss=0.143]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 179/484 [29:48<47:08,  9.27s/it, training_loss=0.185]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 180/484 [29:48<49:44,  9.82s/it, training_loss=0.185]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 180/484 [29:55<49:44,  9.82s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 181/484 [29:55<46:37,  9.23s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  37%|███▋      | 181/484 [30:06<46:37,  9.23s/it, training_loss=0.294]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 182/484 [30:06<49:06,  9.76s/it, training_loss=0.294]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 182/484 [30:15<49:06,  9.76s/it, training_loss=0.241]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 183/484 [30:15<46:42,  9.31s/it, training_loss=0.241]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 183/484 [30:26<46:42,  9.31s/it, training_loss=0.143]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 184/484 [30:26<49:43,  9.94s/it, training_loss=0.143]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 184/484 [30:34<49:43,  9.94s/it, training_loss=0.227]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 185/484 [30:34<46:32,  9.34s/it, training_loss=0.227]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 185/484 [30:45<46:32,  9.34s/it, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 186/484 [30:45<48:19,  9.73s/it, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  38%|███▊      | 186/484 [30:53<48:19,  9.73s/it, training_loss=0.165]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 187/484 [30:53<45:39,  9.22s/it, training_loss=0.165]\u001b[A\n",
            "Epoch 1:  39%|███▊      | 187/484 [31:03<45:39,  9.22s/it, training_loss=0.172]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 188/484 [31:03<47:18,  9.59s/it, training_loss=0.172]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 188/484 [31:11<47:18,  9.59s/it, training_loss=0.247]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 189/484 [31:11<45:02,  9.16s/it, training_loss=0.247]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 189/484 [31:22<45:02,  9.16s/it, training_loss=0.175]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 190/484 [31:22<46:29,  9.49s/it, training_loss=0.175]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 190/484 [31:30<46:29,  9.49s/it, training_loss=0.227]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 191/484 [31:30<44:43,  9.16s/it, training_loss=0.227]\u001b[A\n",
            "Epoch 1:  39%|███▉      | 191/484 [31:40<44:43,  9.16s/it, training_loss=0.225]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 192/484 [31:40<45:57,  9.44s/it, training_loss=0.225]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 192/484 [31:49<45:57,  9.44s/it, training_loss=0.208]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 193/484 [31:49<44:49,  9.24s/it, training_loss=0.208]\u001b[A\n",
            "Epoch 1:  40%|███▉      | 193/484 [31:59<44:49,  9.24s/it, training_loss=0.318]\u001b[A\n",
            "Epoch 1:  40%|████      | 194/484 [31:59<45:28,  9.41s/it, training_loss=0.318]\u001b[A\n",
            "Epoch 1:  40%|████      | 194/484 [32:08<45:28,  9.41s/it, training_loss=0.199]\u001b[A\n",
            "Epoch 1:  40%|████      | 195/484 [32:08<44:44,  9.29s/it, training_loss=0.199]\u001b[A\n",
            "Epoch 1:  40%|████      | 195/484 [32:17<44:44,  9.29s/it, training_loss=0.257]\u001b[A\n",
            "Epoch 1:  40%|████      | 196/484 [32:17<44:52,  9.35s/it, training_loss=0.257]\u001b[A\n",
            "Epoch 1:  40%|████      | 196/484 [32:26<44:52,  9.35s/it, training_loss=0.209]\u001b[A\n",
            "Epoch 1:  41%|████      | 197/484 [32:26<44:33,  9.32s/it, training_loss=0.209]\u001b[A\n",
            "Epoch 1:  41%|████      | 197/484 [32:36<44:33,  9.32s/it, training_loss=0.141]\u001b[A\n",
            "Epoch 1:  41%|████      | 198/484 [32:36<44:20,  9.30s/it, training_loss=0.141]\u001b[A\n",
            "Epoch 1:  41%|████      | 198/484 [32:47<44:20,  9.30s/it, training_loss=0.190]\u001b[A\n",
            "Epoch 1:  41%|████      | 199/484 [32:47<47:42, 10.04s/it, training_loss=0.190]\u001b[A\n",
            "Epoch 1:  41%|████      | 199/484 [32:58<47:42, 10.04s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 200/484 [32:58<47:41, 10.08s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  41%|████▏     | 200/484 [33:06<47:41, 10.08s/it, training_loss=0.135]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 201/484 [33:06<45:44,  9.70s/it, training_loss=0.135]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 201/484 [33:16<45:44,  9.70s/it, training_loss=0.275]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 202/484 [33:16<45:46,  9.74s/it, training_loss=0.275]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 202/484 [33:25<45:46,  9.74s/it, training_loss=0.175]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 203/484 [33:25<44:41,  9.54s/it, training_loss=0.175]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 203/484 [33:35<44:41,  9.54s/it, training_loss=0.366]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 204/484 [33:35<44:12,  9.47s/it, training_loss=0.366]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 204/484 [33:44<44:12,  9.47s/it, training_loss=0.148]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 205/484 [33:44<43:51,  9.43s/it, training_loss=0.148]\u001b[A\n",
            "Epoch 1:  42%|████▏     | 205/484 [33:53<43:51,  9.43s/it, training_loss=0.218]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 206/484 [33:53<43:24,  9.37s/it, training_loss=0.218]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 206/484 [34:03<43:24,  9.37s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 207/484 [34:03<43:48,  9.49s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 207/484 [34:12<43:48,  9.49s/it, training_loss=0.223]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 208/484 [34:12<42:55,  9.33s/it, training_loss=0.223]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 208/484 [34:22<42:55,  9.33s/it, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 209/484 [34:22<43:42,  9.54s/it, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 209/484 [34:31<43:42,  9.54s/it, training_loss=0.127]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 210/484 [34:31<42:25,  9.29s/it, training_loss=0.127]\u001b[A\n",
            "Epoch 1:  43%|████▎     | 210/484 [34:41<42:25,  9.29s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 211/484 [34:41<43:32,  9.57s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  44%|████▎     | 211/484 [34:49<43:32,  9.57s/it, training_loss=0.307]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 212/484 [34:49<41:51,  9.23s/it, training_loss=0.307]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 212/484 [35:00<41:51,  9.23s/it, training_loss=0.214]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 213/484 [35:00<43:17,  9.59s/it, training_loss=0.214]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 213/484 [35:08<43:17,  9.59s/it, training_loss=0.197]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 214/484 [35:08<41:24,  9.20s/it, training_loss=0.197]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 214/484 [35:19<41:24,  9.20s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 215/484 [35:19<43:06,  9.61s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  44%|████▍     | 215/484 [35:27<43:06,  9.61s/it, training_loss=0.348]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 216/484 [35:27<40:49,  9.14s/it, training_loss=0.348]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 216/484 [35:37<40:49,  9.14s/it, training_loss=0.226]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 217/484 [35:37<42:48,  9.62s/it, training_loss=0.226]\u001b[A\n",
            "Epoch 1:  45%|████▍     | 217/484 [35:45<42:48,  9.62s/it, training_loss=0.162]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 218/484 [35:45<40:30,  9.14s/it, training_loss=0.162]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 218/484 [35:56<40:30,  9.14s/it, training_loss=0.110]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 219/484 [35:56<42:47,  9.69s/it, training_loss=0.110]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 219/484 [36:04<42:47,  9.69s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 220/484 [36:04<40:13,  9.14s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  45%|████▌     | 220/484 [36:15<40:13,  9.14s/it, training_loss=0.234]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 221/484 [36:15<42:28,  9.69s/it, training_loss=0.234]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 221/484 [36:23<42:28,  9.69s/it, training_loss=0.245]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 222/484 [36:23<39:54,  9.14s/it, training_loss=0.245]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 222/484 [36:34<39:54,  9.14s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 223/484 [36:34<42:07,  9.68s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  46%|████▌     | 223/484 [36:42<42:07,  9.68s/it, training_loss=0.215]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 224/484 [36:42<39:35,  9.13s/it, training_loss=0.215]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 224/484 [36:55<39:35,  9.13s/it, training_loss=0.144]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 225/484 [36:55<44:35, 10.33s/it, training_loss=0.144]\u001b[A\n",
            "Epoch 1:  46%|████▋     | 225/484 [37:04<44:35, 10.33s/it, training_loss=0.181]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 226/484 [37:04<42:34,  9.90s/it, training_loss=0.181]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 226/484 [37:14<42:34,  9.90s/it, training_loss=0.158]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 227/484 [37:14<42:35,  9.94s/it, training_loss=0.158]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 227/484 [37:23<42:35,  9.94s/it, training_loss=0.224]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 228/484 [37:23<40:49,  9.57s/it, training_loss=0.224]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 228/484 [37:33<40:49,  9.57s/it, training_loss=0.258]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 229/484 [37:33<41:27,  9.76s/it, training_loss=0.258]\u001b[A\n",
            "Epoch 1:  47%|████▋     | 229/484 [37:41<41:27,  9.76s/it, training_loss=0.226]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 230/484 [37:41<39:47,  9.40s/it, training_loss=0.226]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 230/484 [37:52<39:47,  9.40s/it, training_loss=0.175]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 231/484 [37:52<41:09,  9.76s/it, training_loss=0.175]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 231/484 [38:00<41:09,  9.76s/it, training_loss=0.214]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 232/484 [38:00<39:03,  9.30s/it, training_loss=0.214]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 232/484 [38:11<39:03,  9.30s/it, training_loss=0.151]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 233/484 [38:11<40:47,  9.75s/it, training_loss=0.151]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 233/484 [38:19<40:47,  9.75s/it, training_loss=0.155]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 234/484 [38:19<38:24,  9.22s/it, training_loss=0.155]\u001b[A\n",
            "Epoch 1:  48%|████▊     | 234/484 [38:30<38:24,  9.22s/it, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 235/484 [38:30<40:21,  9.73s/it, training_loss=0.211]\u001b[A\n",
            "Epoch 1:  49%|████▊     | 235/484 [38:38<40:21,  9.73s/it, training_loss=0.140]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 236/484 [38:38<37:52,  9.17s/it, training_loss=0.140]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 236/484 [38:49<37:52,  9.17s/it, training_loss=0.121]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 237/484 [38:49<39:59,  9.72s/it, training_loss=0.121]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 237/484 [38:57<39:59,  9.72s/it, training_loss=0.183]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 238/484 [38:57<37:30,  9.15s/it, training_loss=0.183]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 238/484 [39:08<37:30,  9.15s/it, training_loss=0.302]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 239/484 [39:08<39:34,  9.69s/it, training_loss=0.302]\u001b[A\n",
            "Epoch 1:  49%|████▉     | 239/484 [39:15<39:34,  9.69s/it, training_loss=0.191]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 240/484 [39:15<37:10,  9.14s/it, training_loss=0.191]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 240/484 [39:26<37:10,  9.14s/it, training_loss=0.215]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 241/484 [39:26<39:13,  9.68s/it, training_loss=0.215]\u001b[A\n",
            "Epoch 1:  50%|████▉     | 241/484 [39:34<39:13,  9.68s/it, training_loss=0.204]\u001b[A\n",
            "Epoch 1:  50%|█████     | 242/484 [39:34<36:45,  9.11s/it, training_loss=0.204]\u001b[A\n",
            "Epoch 1:  50%|█████     | 242/484 [39:45<36:45,  9.11s/it, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  50%|█████     | 243/484 [39:45<38:53,  9.68s/it, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  50%|█████     | 243/484 [39:53<38:53,  9.68s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  50%|█████     | 244/484 [39:53<36:42,  9.18s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  50%|█████     | 244/484 [40:04<36:42,  9.18s/it, training_loss=0.118]\u001b[A\n",
            "Epoch 1:  51%|█████     | 245/484 [40:04<38:20,  9.62s/it, training_loss=0.118]\u001b[A\n",
            "Epoch 1:  51%|█████     | 245/484 [40:12<38:20,  9.62s/it, training_loss=0.166]\u001b[A\n",
            "Epoch 1:  51%|█████     | 246/484 [40:12<36:28,  9.19s/it, training_loss=0.166]\u001b[A\n",
            "Epoch 1:  51%|█████     | 246/484 [40:22<36:28,  9.19s/it, training_loss=0.273]\u001b[A\n",
            "Epoch 1:  51%|█████     | 247/484 [40:22<37:44,  9.56s/it, training_loss=0.273]\u001b[A\n",
            "Epoch 1:  51%|█████     | 247/484 [40:31<37:44,  9.56s/it, training_loss=0.157]\u001b[A\n",
            "Epoch 1:  51%|█████     | 248/484 [40:31<36:14,  9.22s/it, training_loss=0.157]\u001b[A\n",
            "Epoch 1:  51%|█████     | 248/484 [40:41<36:14,  9.22s/it, training_loss=0.277]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 249/484 [40:41<37:07,  9.48s/it, training_loss=0.277]\u001b[A\n",
            "Epoch 1:  51%|█████▏    | 249/484 [40:50<37:07,  9.48s/it, training_loss=0.378]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 250/484 [40:51<37:03,  9.50s/it, training_loss=0.378]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 250/484 [41:03<37:03,  9.50s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 251/484 [41:03<40:08, 10.34s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 251/484 [41:11<40:08, 10.34s/it, training_loss=0.092]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 252/484 [41:11<37:04,  9.59s/it, training_loss=0.092]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 252/484 [41:22<37:04,  9.59s/it, training_loss=0.068]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 253/484 [41:22<38:47, 10.08s/it, training_loss=0.068]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 253/484 [41:30<38:47, 10.08s/it, training_loss=0.095]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 254/484 [41:30<36:30,  9.53s/it, training_loss=0.095]\u001b[A\n",
            "Epoch 1:  52%|█████▏    | 254/484 [41:41<36:30,  9.53s/it, training_loss=0.143]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 255/484 [41:41<37:57,  9.94s/it, training_loss=0.143]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 255/484 [41:49<37:57,  9.94s/it, training_loss=0.189]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 256/484 [41:49<35:36,  9.37s/it, training_loss=0.189]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 256/484 [42:00<35:36,  9.37s/it, training_loss=0.190]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 257/484 [42:00<36:48,  9.73s/it, training_loss=0.190]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 257/484 [42:08<36:48,  9.73s/it, training_loss=0.235]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 258/484 [42:08<34:57,  9.28s/it, training_loss=0.235]\u001b[A\n",
            "Epoch 1:  53%|█████▎    | 258/484 [42:18<34:57,  9.28s/it, training_loss=0.185]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 259/484 [42:18<35:54,  9.58s/it, training_loss=0.185]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 259/484 [42:27<35:54,  9.58s/it, training_loss=0.160]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 260/484 [42:27<34:33,  9.26s/it, training_loss=0.160]\u001b[A\n",
            "Epoch 1:  54%|█████▎    | 260/484 [42:37<34:33,  9.26s/it, training_loss=0.160]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 261/484 [42:37<35:11,  9.47s/it, training_loss=0.160]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 261/484 [42:45<35:11,  9.47s/it, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 262/484 [42:45<34:13,  9.25s/it, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 262/484 [42:55<34:13,  9.25s/it, training_loss=0.200]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 263/484 [42:55<34:39,  9.41s/it, training_loss=0.200]\u001b[A\n",
            "Epoch 1:  54%|█████▍    | 263/484 [43:04<34:39,  9.41s/it, training_loss=0.333]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 264/484 [43:04<34:05,  9.30s/it, training_loss=0.333]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 264/484 [43:14<34:05,  9.30s/it, training_loss=0.171]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 265/484 [43:14<34:17,  9.39s/it, training_loss=0.171]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 265/484 [43:23<34:17,  9.39s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 266/484 [43:23<34:09,  9.40s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  55%|█████▍    | 266/484 [43:32<34:09,  9.40s/it, training_loss=0.108]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 267/484 [43:32<33:53,  9.37s/it, training_loss=0.108]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 267/484 [43:42<33:53,  9.37s/it, training_loss=0.196]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 268/484 [43:42<34:08,  9.49s/it, training_loss=0.196]\u001b[A\n",
            "Epoch 1:  55%|█████▌    | 268/484 [43:51<34:08,  9.49s/it, training_loss=0.204]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 269/484 [43:51<33:38,  9.39s/it, training_loss=0.204]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 269/484 [44:01<33:38,  9.39s/it, training_loss=0.223]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 270/484 [44:01<34:09,  9.58s/it, training_loss=0.223]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 270/484 [44:10<34:09,  9.58s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 271/484 [44:10<33:18,  9.38s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 271/484 [44:21<33:18,  9.38s/it, training_loss=0.164]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 272/484 [44:21<34:07,  9.66s/it, training_loss=0.164]\u001b[A\n",
            "Epoch 1:  56%|█████▌    | 272/484 [44:29<34:07,  9.66s/it, training_loss=0.274]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 273/484 [44:29<32:49,  9.34s/it, training_loss=0.274]\u001b[A\n",
            "Epoch 1:  56%|█████▋    | 273/484 [44:40<32:49,  9.34s/it, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 274/484 [44:40<33:52,  9.68s/it, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 274/484 [44:48<33:52,  9.68s/it, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 275/484 [44:48<32:24,  9.30s/it, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 275/484 [45:01<32:24,  9.30s/it, training_loss=0.194]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 276/484 [45:01<35:31, 10.25s/it, training_loss=0.194]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 276/484 [45:10<35:31, 10.25s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 277/484 [45:10<34:55, 10.12s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 277/484 [45:20<34:55, 10.12s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 278/484 [45:20<33:57,  9.89s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  57%|█████▋    | 278/484 [45:30<33:57,  9.89s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 279/484 [45:30<34:04,  9.97s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 279/484 [45:43<34:04,  9.97s/it, training_loss=0.179]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 280/484 [45:43<36:46, 10.81s/it, training_loss=0.179]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 280/484 [45:51<36:46, 10.81s/it, training_loss=0.114]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 281/484 [45:51<34:07, 10.09s/it, training_loss=0.114]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 281/484 [46:02<34:07, 10.09s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 282/484 [46:02<34:49, 10.34s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 282/484 [46:10<34:49, 10.34s/it, training_loss=0.127]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 283/484 [46:10<32:16,  9.63s/it, training_loss=0.127]\u001b[A\n",
            "Epoch 1:  58%|█████▊    | 283/484 [46:21<32:16,  9.63s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 284/484 [46:21<33:27, 10.04s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  59%|█████▊    | 284/484 [46:29<33:27, 10.04s/it, training_loss=0.164]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 285/484 [46:29<31:06,  9.38s/it, training_loss=0.164]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 285/484 [46:40<31:06,  9.38s/it, training_loss=0.102]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 286/484 [46:40<32:36,  9.88s/it, training_loss=0.102]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 286/484 [46:48<32:36,  9.88s/it, training_loss=0.174]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 287/484 [46:48<30:27,  9.27s/it, training_loss=0.174]\u001b[A\n",
            "Epoch 1:  59%|█████▉    | 287/484 [46:59<30:27,  9.27s/it, training_loss=0.200]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 288/484 [46:59<31:54,  9.77s/it, training_loss=0.200]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 288/484 [47:07<31:54,  9.77s/it, training_loss=0.129]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 289/484 [47:07<29:54,  9.20s/it, training_loss=0.129]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 289/484 [47:18<29:54,  9.20s/it, training_loss=0.174]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 290/484 [47:18<31:29,  9.74s/it, training_loss=0.174]\u001b[A\n",
            "Epoch 1:  60%|█████▉    | 290/484 [47:25<31:29,  9.74s/it, training_loss=0.091]\u001b[A\n",
            "Epoch 1:  60%|██████    | 291/484 [47:26<29:37,  9.21s/it, training_loss=0.091]\u001b[A\n",
            "Epoch 1:  60%|██████    | 291/484 [47:36<29:37,  9.21s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  60%|██████    | 292/484 [47:36<31:00,  9.69s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  60%|██████    | 292/484 [47:45<31:00,  9.69s/it, training_loss=0.094]\u001b[A\n",
            "Epoch 1:  61%|██████    | 293/484 [47:45<29:27,  9.26s/it, training_loss=0.094]\u001b[A\n",
            "Epoch 1:  61%|██████    | 293/484 [47:55<29:27,  9.26s/it, training_loss=0.150]\u001b[A\n",
            "Epoch 1:  61%|██████    | 294/484 [47:55<30:30,  9.63s/it, training_loss=0.150]\u001b[A\n",
            "Epoch 1:  61%|██████    | 294/484 [48:04<30:30,  9.63s/it, training_loss=0.275]\u001b[A\n",
            "Epoch 1:  61%|██████    | 295/484 [48:04<29:25,  9.34s/it, training_loss=0.275]\u001b[A\n",
            "Epoch 1:  61%|██████    | 295/484 [48:14<29:25,  9.34s/it, training_loss=0.067]\u001b[A\n",
            "Epoch 1:  61%|██████    | 296/484 [48:14<29:57,  9.56s/it, training_loss=0.067]\u001b[A\n",
            "Epoch 1:  61%|██████    | 296/484 [48:23<29:57,  9.56s/it, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 297/484 [48:23<29:00,  9.31s/it, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  61%|██████▏   | 297/484 [48:32<29:00,  9.31s/it, training_loss=0.135]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 298/484 [48:32<29:16,  9.45s/it, training_loss=0.135]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 298/484 [48:41<29:16,  9.45s/it, training_loss=0.201]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 299/484 [48:41<28:42,  9.31s/it, training_loss=0.201]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 299/484 [48:51<28:42,  9.31s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 300/484 [48:51<28:40,  9.35s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 300/484 [49:01<28:40,  9.35s/it, training_loss=0.107]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 301/484 [49:01<29:31,  9.68s/it, training_loss=0.107]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 301/484 [49:13<29:31,  9.68s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 302/484 [49:13<30:54, 10.19s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  62%|██████▏   | 302/484 [49:21<30:54, 10.19s/it, training_loss=0.175]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 303/484 [49:21<28:43,  9.52s/it, training_loss=0.175]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 303/484 [49:31<28:43,  9.52s/it, training_loss=0.099]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 304/484 [49:31<29:32,  9.85s/it, training_loss=0.099]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 304/484 [49:39<29:32,  9.85s/it, training_loss=0.176]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 305/484 [49:39<27:51,  9.34s/it, training_loss=0.176]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 305/484 [49:50<27:51,  9.34s/it, training_loss=0.212]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 306/484 [49:50<28:42,  9.67s/it, training_loss=0.212]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 306/484 [49:58<28:42,  9.67s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 307/484 [49:58<27:27,  9.31s/it, training_loss=0.134]\u001b[A\n",
            "Epoch 1:  63%|██████▎   | 307/484 [50:08<27:27,  9.31s/it, training_loss=0.158]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 308/484 [50:08<27:58,  9.53s/it, training_loss=0.158]\u001b[A\n",
            "Epoch 1:  64%|██████▎   | 308/484 [50:17<27:58,  9.53s/it, training_loss=0.208]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 309/484 [50:17<27:07,  9.30s/it, training_loss=0.208]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 309/484 [50:27<27:07,  9.30s/it, training_loss=0.196]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 310/484 [50:27<27:22,  9.44s/it, training_loss=0.196]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 310/484 [50:36<27:22,  9.44s/it, training_loss=0.160]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 311/484 [50:36<26:47,  9.29s/it, training_loss=0.160]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 311/484 [50:45<26:47,  9.29s/it, training_loss=0.201]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 312/484 [50:45<26:51,  9.37s/it, training_loss=0.201]\u001b[A\n",
            "Epoch 1:  64%|██████▍   | 312/484 [50:55<26:51,  9.37s/it, training_loss=0.101]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 313/484 [50:55<26:37,  9.34s/it, training_loss=0.101]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 313/484 [51:04<26:37,  9.34s/it, training_loss=0.103]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 314/484 [51:04<26:20,  9.30s/it, training_loss=0.103]\u001b[A\n",
            "Epoch 1:  65%|██████▍   | 314/484 [51:13<26:20,  9.30s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 315/484 [51:13<26:23,  9.37s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 315/484 [51:22<26:23,  9.37s/it, training_loss=0.186]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 316/484 [51:22<25:54,  9.25s/it, training_loss=0.186]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 316/484 [51:32<25:54,  9.25s/it, training_loss=0.170]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 317/484 [51:32<26:14,  9.43s/it, training_loss=0.170]\u001b[A\n",
            "Epoch 1:  65%|██████▌   | 317/484 [51:41<26:14,  9.43s/it, training_loss=0.186]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 318/484 [51:41<25:33,  9.24s/it, training_loss=0.186]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 318/484 [51:51<25:33,  9.24s/it, training_loss=0.191]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 319/484 [51:51<26:08,  9.51s/it, training_loss=0.191]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 319/484 [52:00<26:08,  9.51s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 320/484 [52:00<25:15,  9.24s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  66%|██████▌   | 320/484 [52:10<25:15,  9.24s/it, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 321/484 [52:10<25:55,  9.54s/it, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  66%|██████▋   | 321/484 [52:18<25:55,  9.54s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 322/484 [52:18<24:49,  9.19s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 322/484 [52:29<24:49,  9.19s/it, training_loss=0.099]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 323/484 [52:29<25:39,  9.56s/it, training_loss=0.099]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 323/484 [52:37<25:39,  9.56s/it, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 324/484 [52:37<24:25,  9.16s/it, training_loss=0.161]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 324/484 [52:48<24:25,  9.16s/it, training_loss=0.095]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 325/484 [52:48<25:26,  9.60s/it, training_loss=0.095]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 325/484 [52:56<25:26,  9.60s/it, training_loss=0.165]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 326/484 [52:56<24:02,  9.13s/it, training_loss=0.165]\u001b[A\n",
            "Epoch 1:  67%|██████▋   | 326/484 [53:07<24:02,  9.13s/it, training_loss=0.118]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 327/484 [53:07<25:35,  9.78s/it, training_loss=0.118]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 327/484 [53:18<25:35,  9.78s/it, training_loss=0.155]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 328/484 [53:18<26:07, 10.05s/it, training_loss=0.155]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 328/484 [53:27<26:07, 10.05s/it, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 329/484 [53:27<25:51, 10.01s/it, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 329/484 [53:36<25:51, 10.01s/it, training_loss=0.153]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 330/484 [53:36<24:55,  9.71s/it, training_loss=0.153]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 330/484 [53:47<24:55,  9.71s/it, training_loss=0.091]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 331/484 [53:47<25:02,  9.82s/it, training_loss=0.091]\u001b[A\n",
            "Epoch 1:  68%|██████▊   | 331/484 [54:03<25:02,  9.82s/it, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 332/484 [54:03<29:53, 11.80s/it, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  69%|██████▊   | 332/484 [54:14<29:53, 11.80s/it, training_loss=0.183]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 333/484 [54:14<29:08, 11.58s/it, training_loss=0.183]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 333/484 [54:24<29:08, 11.58s/it, training_loss=0.068]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 334/484 [54:24<27:37, 11.05s/it, training_loss=0.068]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 334/484 [54:33<27:37, 11.05s/it, training_loss=0.099]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 335/484 [54:33<25:48, 10.39s/it, training_loss=0.099]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 335/484 [54:43<25:48, 10.39s/it, training_loss=0.122]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 336/484 [54:43<25:21, 10.28s/it, training_loss=0.122]\u001b[A\n",
            "Epoch 1:  69%|██████▉   | 336/484 [54:51<25:21, 10.28s/it, training_loss=0.111]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 337/484 [54:51<24:00,  9.80s/it, training_loss=0.111]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 337/484 [55:02<24:00,  9.80s/it, training_loss=0.144]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 338/484 [55:02<24:06,  9.91s/it, training_loss=0.144]\u001b[A\n",
            "Epoch 1:  70%|██████▉   | 338/484 [55:10<24:06,  9.91s/it, training_loss=0.181]\u001b[A\n",
            "Epoch 1:  70%|███████   | 339/484 [55:10<22:52,  9.47s/it, training_loss=0.181]\u001b[A\n",
            "Epoch 1:  70%|███████   | 339/484 [55:20<22:52,  9.47s/it, training_loss=0.045]\u001b[A\n",
            "Epoch 1:  70%|███████   | 340/484 [55:20<23:22,  9.74s/it, training_loss=0.045]\u001b[A\n",
            "Epoch 1:  70%|███████   | 340/484 [55:29<23:22,  9.74s/it, training_loss=0.179]\u001b[A\n",
            "Epoch 1:  70%|███████   | 341/484 [55:29<22:13,  9.33s/it, training_loss=0.179]\u001b[A\n",
            "Epoch 1:  70%|███████   | 341/484 [55:41<22:13,  9.33s/it, training_loss=0.084]\u001b[A\n",
            "Epoch 1:  71%|███████   | 342/484 [55:41<23:51, 10.08s/it, training_loss=0.084]\u001b[A\n",
            "Epoch 1:  71%|███████   | 342/484 [55:49<23:51, 10.08s/it, training_loss=0.110]\u001b[A\n",
            "Epoch 1:  71%|███████   | 343/484 [55:49<22:11,  9.45s/it, training_loss=0.110]\u001b[A\n",
            "Epoch 1:  71%|███████   | 343/484 [55:59<22:11,  9.45s/it, training_loss=0.216]\u001b[A\n",
            "Epoch 1:  71%|███████   | 344/484 [56:00<23:05,  9.90s/it, training_loss=0.216]\u001b[A\n",
            "Epoch 1:  71%|███████   | 344/484 [56:07<23:05,  9.90s/it, training_loss=0.257]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 345/484 [56:07<21:31,  9.29s/it, training_loss=0.257]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 345/484 [56:18<21:31,  9.29s/it, training_loss=0.088]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 346/484 [56:18<22:35,  9.82s/it, training_loss=0.088]\u001b[A\n",
            "Epoch 1:  71%|███████▏  | 346/484 [56:26<22:35,  9.82s/it, training_loss=0.124]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 347/484 [56:26<21:03,  9.22s/it, training_loss=0.124]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 347/484 [56:42<21:03,  9.22s/it, training_loss=0.122]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 348/484 [56:42<25:22, 11.20s/it, training_loss=0.122]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 348/484 [56:59<25:22, 11.20s/it, training_loss=0.138]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 349/484 [56:59<28:55, 12.86s/it, training_loss=0.138]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 349/484 [57:07<28:55, 12.86s/it, training_loss=0.113]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 350/484 [57:07<25:49, 11.56s/it, training_loss=0.113]\u001b[A\n",
            "Epoch 1:  72%|███████▏  | 350/484 [57:20<25:49, 11.56s/it, training_loss=0.166]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 351/484 [57:20<26:30, 11.96s/it, training_loss=0.166]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 351/484 [57:31<26:30, 11.96s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 352/484 [57:31<25:43, 11.69s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 352/484 [57:40<25:43, 11.69s/it, training_loss=0.126]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 353/484 [57:40<23:36, 10.81s/it, training_loss=0.126]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 353/484 [57:50<23:36, 10.81s/it, training_loss=0.172]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 354/484 [57:50<23:02, 10.64s/it, training_loss=0.172]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 354/484 [57:59<23:02, 10.64s/it, training_loss=0.145]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 355/484 [57:59<21:33, 10.03s/it, training_loss=0.145]\u001b[A\n",
            "Epoch 1:  73%|███████▎  | 355/484 [58:09<21:33, 10.03s/it, training_loss=0.104]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 356/484 [58:09<21:38, 10.15s/it, training_loss=0.104]\u001b[A\n",
            "Epoch 1:  74%|███████▎  | 356/484 [58:18<21:38, 10.15s/it, training_loss=0.351]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 357/484 [58:18<20:19,  9.60s/it, training_loss=0.351]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 357/484 [58:28<20:19,  9.60s/it, training_loss=0.133]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 358/484 [58:28<20:51,  9.93s/it, training_loss=0.133]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 358/484 [58:36<20:51,  9.93s/it, training_loss=0.173]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 359/484 [58:36<19:29,  9.36s/it, training_loss=0.173]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 359/484 [58:47<19:29,  9.36s/it, training_loss=0.085]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 360/484 [58:47<20:18,  9.83s/it, training_loss=0.085]\u001b[A\n",
            "Epoch 1:  74%|███████▍  | 360/484 [58:55<20:18,  9.83s/it, training_loss=0.129]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 361/484 [58:55<18:56,  9.24s/it, training_loss=0.129]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 361/484 [59:06<18:56,  9.24s/it, training_loss=0.385]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 362/484 [59:06<19:50,  9.76s/it, training_loss=0.385]\u001b[A\n",
            "Epoch 1:  75%|███████▍  | 362/484 [59:15<19:50,  9.76s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 363/484 [59:15<19:03,  9.45s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 363/484 [59:26<19:03,  9.45s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 364/484 [59:26<19:49,  9.91s/it, training_loss=0.177]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 364/484 [59:34<19:49,  9.91s/it, training_loss=0.069]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 365/484 [59:34<18:24,  9.28s/it, training_loss=0.069]\u001b[A\n",
            "Epoch 1:  75%|███████▌  | 365/484 [59:45<18:24,  9.28s/it, training_loss=0.068]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 366/484 [59:45<19:15,  9.79s/it, training_loss=0.068]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 366/484 [59:53<19:15,  9.79s/it, training_loss=0.135]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 367/484 [59:53<18:01,  9.24s/it, training_loss=0.135]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 367/484 [1:00:03<18:01,  9.24s/it, training_loss=0.160]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 368/484 [1:00:03<18:42,  9.68s/it, training_loss=0.160]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 368/484 [1:00:11<18:42,  9.68s/it, training_loss=0.176]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 369/484 [1:00:11<17:39,  9.22s/it, training_loss=0.176]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 369/484 [1:00:22<17:39,  9.22s/it, training_loss=0.121]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 370/484 [1:00:22<18:12,  9.58s/it, training_loss=0.121]\u001b[A\n",
            "Epoch 1:  76%|███████▋  | 370/484 [1:00:30<18:12,  9.58s/it, training_loss=0.073]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 371/484 [1:00:30<17:23,  9.23s/it, training_loss=0.073]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 371/484 [1:00:40<17:23,  9.23s/it, training_loss=0.178]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 372/484 [1:00:40<17:44,  9.50s/it, training_loss=0.178]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 372/484 [1:00:49<17:44,  9.50s/it, training_loss=0.100]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 373/484 [1:00:49<17:06,  9.25s/it, training_loss=0.100]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 373/484 [1:00:59<17:06,  9.25s/it, training_loss=0.049]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 374/484 [1:00:59<17:17,  9.43s/it, training_loss=0.049]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 374/484 [1:01:08<17:17,  9.43s/it, training_loss=0.186]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 375/484 [1:01:08<16:45,  9.23s/it, training_loss=0.186]\u001b[A\n",
            "Epoch 1:  77%|███████▋  | 375/484 [1:01:17<16:45,  9.23s/it, training_loss=0.253]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 376/484 [1:01:17<16:53,  9.39s/it, training_loss=0.253]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 376/484 [1:01:28<16:53,  9.39s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 377/484 [1:01:28<17:09,  9.62s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 377/484 [1:01:39<17:09,  9.62s/it, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 378/484 [1:01:39<18:07, 10.26s/it, training_loss=0.195]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 378/484 [1:01:47<18:07, 10.26s/it, training_loss=0.202]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 379/484 [1:01:47<16:43,  9.55s/it, training_loss=0.202]\u001b[A\n",
            "Epoch 1:  78%|███████▊  | 379/484 [1:01:58<16:43,  9.55s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 380/484 [1:01:58<17:17,  9.98s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 380/484 [1:02:06<17:17,  9.98s/it, training_loss=0.153]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 381/484 [1:02:06<16:02,  9.35s/it, training_loss=0.153]\u001b[A\n",
            "Epoch 1:  79%|███████▊  | 381/484 [1:02:17<16:02,  9.35s/it, training_loss=0.251]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 382/484 [1:02:17<16:39,  9.80s/it, training_loss=0.251]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 382/484 [1:02:25<16:39,  9.80s/it, training_loss=0.130]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 383/484 [1:02:25<15:34,  9.25s/it, training_loss=0.130]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 383/484 [1:02:36<15:34,  9.25s/it, training_loss=0.059]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 384/484 [1:02:36<16:04,  9.65s/it, training_loss=0.059]\u001b[A\n",
            "Epoch 1:  79%|███████▉  | 384/484 [1:02:44<16:04,  9.65s/it, training_loss=0.231]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 385/484 [1:02:44<15:12,  9.21s/it, training_loss=0.231]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 385/484 [1:02:54<15:12,  9.21s/it, training_loss=0.192]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 386/484 [1:02:54<15:33,  9.53s/it, training_loss=0.192]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 386/484 [1:03:02<15:33,  9.53s/it, training_loss=0.300]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 387/484 [1:03:02<14:53,  9.21s/it, training_loss=0.300]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 387/484 [1:03:13<14:53,  9.21s/it, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  80%|████████  | 388/484 [1:03:13<15:10,  9.48s/it, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  80%|████████  | 388/484 [1:03:21<15:10,  9.48s/it, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  80%|████████  | 389/484 [1:03:21<14:42,  9.29s/it, training_loss=0.236]\u001b[A\n",
            "Epoch 1:  80%|████████  | 389/484 [1:03:31<14:42,  9.29s/it, training_loss=0.151]\u001b[A\n",
            "Epoch 1:  81%|████████  | 390/484 [1:03:31<14:47,  9.44s/it, training_loss=0.151]\u001b[A\n",
            "Epoch 1:  81%|████████  | 390/484 [1:03:40<14:47,  9.44s/it, training_loss=0.140]\u001b[A\n",
            "Epoch 1:  81%|████████  | 391/484 [1:03:40<14:28,  9.34s/it, training_loss=0.140]\u001b[A\n",
            "Epoch 1:  81%|████████  | 391/484 [1:03:50<14:28,  9.34s/it, training_loss=0.106]\u001b[A\n",
            "Epoch 1:  81%|████████  | 392/484 [1:03:50<14:26,  9.42s/it, training_loss=0.106]\u001b[A\n",
            "Epoch 1:  81%|████████  | 392/484 [1:04:00<14:26,  9.42s/it, training_loss=0.077]\u001b[A\n",
            "Epoch 1:  81%|████████  | 393/484 [1:04:00<14:22,  9.48s/it, training_loss=0.077]\u001b[A\n",
            "Epoch 1:  81%|████████  | 393/484 [1:04:09<14:22,  9.48s/it, training_loss=0.196]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 394/484 [1:04:09<14:05,  9.39s/it, training_loss=0.196]\u001b[A\n",
            "Epoch 1:  81%|████████▏ | 394/484 [1:04:19<14:05,  9.39s/it, training_loss=0.148]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 395/484 [1:04:19<14:10,  9.56s/it, training_loss=0.148]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 395/484 [1:04:28<14:10,  9.56s/it, training_loss=0.061]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 396/484 [1:04:28<13:44,  9.37s/it, training_loss=0.061]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 396/484 [1:04:38<13:44,  9.37s/it, training_loss=0.190]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 397/484 [1:04:38<13:57,  9.62s/it, training_loss=0.190]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 397/484 [1:04:46<13:57,  9.62s/it, training_loss=0.202]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 398/484 [1:04:46<13:21,  9.31s/it, training_loss=0.202]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 398/484 [1:04:57<13:21,  9.31s/it, training_loss=0.180]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 399/484 [1:04:57<13:38,  9.63s/it, training_loss=0.180]\u001b[A\n",
            "Epoch 1:  82%|████████▏ | 399/484 [1:05:05<13:38,  9.63s/it, training_loss=0.108]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 400/484 [1:05:05<12:57,  9.25s/it, training_loss=0.108]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 400/484 [1:05:16<12:57,  9.25s/it, training_loss=0.230]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 401/484 [1:05:16<13:21,  9.65s/it, training_loss=0.230]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 401/484 [1:05:24<13:21,  9.65s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 402/484 [1:05:24<12:35,  9.21s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 402/484 [1:05:35<12:35,  9.21s/it, training_loss=0.119]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 403/484 [1:05:35<13:03,  9.67s/it, training_loss=0.119]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 403/484 [1:05:47<13:03,  9.67s/it, training_loss=0.119]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 404/484 [1:05:47<14:01, 10.51s/it, training_loss=0.119]\u001b[A\n",
            "Epoch 1:  83%|████████▎ | 404/484 [1:05:57<14:01, 10.51s/it, training_loss=0.095]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 405/484 [1:05:57<13:41, 10.40s/it, training_loss=0.095]\u001b[A\n",
            "Epoch 1:  84%|████████▎ | 405/484 [1:06:06<13:41, 10.40s/it, training_loss=0.130]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 406/484 [1:06:06<12:56,  9.95s/it, training_loss=0.130]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 406/484 [1:06:16<12:56,  9.95s/it, training_loss=0.233]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 407/484 [1:06:16<12:40,  9.88s/it, training_loss=0.233]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 407/484 [1:06:25<12:40,  9.88s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 408/484 [1:06:25<12:14,  9.66s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  84%|████████▍ | 408/484 [1:06:34<12:14,  9.66s/it, training_loss=0.104]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 409/484 [1:06:34<12:00,  9.60s/it, training_loss=0.104]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 409/484 [1:06:44<12:00,  9.60s/it, training_loss=0.076]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 410/484 [1:06:44<11:44,  9.52s/it, training_loss=0.076]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 410/484 [1:06:53<11:44,  9.52s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 411/484 [1:06:53<11:28,  9.43s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 411/484 [1:07:03<11:28,  9.43s/it, training_loss=0.082]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 412/484 [1:07:03<11:25,  9.52s/it, training_loss=0.082]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 412/484 [1:07:12<11:25,  9.52s/it, training_loss=0.101]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 413/484 [1:07:12<11:03,  9.35s/it, training_loss=0.101]\u001b[A\n",
            "Epoch 1:  85%|████████▌ | 413/484 [1:07:22<11:03,  9.35s/it, training_loss=0.060]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 414/484 [1:07:22<11:08,  9.55s/it, training_loss=0.060]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 414/484 [1:07:30<11:08,  9.55s/it, training_loss=0.121]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 415/484 [1:07:30<10:42,  9.30s/it, training_loss=0.121]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 415/484 [1:07:41<10:42,  9.30s/it, training_loss=0.205]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 416/484 [1:07:41<10:49,  9.55s/it, training_loss=0.205]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 416/484 [1:07:49<10:49,  9.55s/it, training_loss=0.098]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 417/484 [1:07:49<10:22,  9.29s/it, training_loss=0.098]\u001b[A\n",
            "Epoch 1:  86%|████████▌ | 417/484 [1:08:00<10:22,  9.29s/it, training_loss=0.089]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 418/484 [1:08:00<10:34,  9.61s/it, training_loss=0.089]\u001b[A\n",
            "Epoch 1:  86%|████████▋ | 418/484 [1:08:08<10:34,  9.61s/it, training_loss=0.152]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 419/484 [1:08:08<10:01,  9.25s/it, training_loss=0.152]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 419/484 [1:08:19<10:01,  9.25s/it, training_loss=0.090]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 420/484 [1:08:19<10:17,  9.65s/it, training_loss=0.090]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 420/484 [1:08:27<10:17,  9.65s/it, training_loss=0.115]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 421/484 [1:08:27<09:39,  9.20s/it, training_loss=0.115]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 421/484 [1:08:38<09:39,  9.20s/it, training_loss=0.068]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 422/484 [1:08:38<10:00,  9.68s/it, training_loss=0.068]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 422/484 [1:08:45<10:00,  9.68s/it, training_loss=0.132]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 423/484 [1:08:45<09:17,  9.14s/it, training_loss=0.132]\u001b[A\n",
            "Epoch 1:  87%|████████▋ | 423/484 [1:08:56<09:17,  9.14s/it, training_loss=0.113]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 424/484 [1:08:56<09:40,  9.68s/it, training_loss=0.113]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 424/484 [1:09:04<09:40,  9.68s/it, training_loss=0.172]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 425/484 [1:09:04<08:57,  9.11s/it, training_loss=0.172]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 425/484 [1:09:15<08:57,  9.11s/it, training_loss=0.071]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 426/484 [1:09:15<09:20,  9.66s/it, training_loss=0.071]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 426/484 [1:09:23<09:20,  9.66s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 427/484 [1:09:23<08:39,  9.12s/it, training_loss=0.203]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 427/484 [1:09:34<08:39,  9.12s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 428/484 [1:09:34<09:02,  9.68s/it, training_loss=0.210]\u001b[A\n",
            "Epoch 1:  88%|████████▊ | 428/484 [1:09:42<09:02,  9.68s/it, training_loss=0.145]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 429/484 [1:09:42<08:22,  9.13s/it, training_loss=0.145]\u001b[A\n",
            "Epoch 1:  89%|████████▊ | 429/484 [1:09:53<08:22,  9.13s/it, training_loss=0.274]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 430/484 [1:09:53<08:44,  9.71s/it, training_loss=0.274]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 430/484 [1:10:05<08:44,  9.71s/it, training_loss=0.117]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 431/484 [1:10:05<09:18, 10.53s/it, training_loss=0.117]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 431/484 [1:10:14<09:18, 10.53s/it, training_loss=0.178]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 432/484 [1:10:14<08:44, 10.09s/it, training_loss=0.178]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 432/484 [1:10:24<08:44, 10.09s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 433/484 [1:10:24<08:29, 10.00s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 433/484 [1:10:33<08:29, 10.00s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 434/484 [1:10:33<08:03,  9.67s/it, training_loss=0.128]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 434/484 [1:10:43<08:03,  9.67s/it, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 435/484 [1:10:43<07:58,  9.76s/it, training_loss=0.136]\u001b[A\n",
            "Epoch 1:  90%|████████▉ | 435/484 [1:10:52<07:58,  9.76s/it, training_loss=0.130]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 436/484 [1:10:52<07:32,  9.43s/it, training_loss=0.130]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 436/484 [1:11:02<07:32,  9.43s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 437/484 [1:11:02<07:32,  9.64s/it, training_loss=0.169]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 437/484 [1:11:10<07:32,  9.64s/it, training_loss=0.294]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 438/484 [1:11:10<07:08,  9.32s/it, training_loss=0.294]\u001b[A\n",
            "Epoch 1:  90%|█████████ | 438/484 [1:11:21<07:08,  9.32s/it, training_loss=0.246]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 439/484 [1:11:21<07:12,  9.62s/it, training_loss=0.246]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 439/484 [1:11:29<07:12,  9.62s/it, training_loss=0.093]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 440/484 [1:11:29<06:47,  9.27s/it, training_loss=0.093]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 440/484 [1:11:40<06:47,  9.27s/it, training_loss=0.180]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 441/484 [1:11:40<06:54,  9.63s/it, training_loss=0.180]\u001b[A\n",
            "Epoch 1:  91%|█████████ | 441/484 [1:11:48<06:54,  9.63s/it, training_loss=0.139]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 442/484 [1:11:48<06:27,  9.22s/it, training_loss=0.139]\u001b[A\n",
            "Epoch 1:  91%|█████████▏| 442/484 [1:11:59<06:27,  9.22s/it, training_loss=0.087]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 443/484 [1:11:59<06:36,  9.66s/it, training_loss=0.087]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 443/484 [1:12:07<06:36,  9.66s/it, training_loss=0.239]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 444/484 [1:12:07<06:07,  9.18s/it, training_loss=0.239]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 444/484 [1:12:18<06:07,  9.18s/it, training_loss=0.230]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 445/484 [1:12:18<06:17,  9.69s/it, training_loss=0.230]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 445/484 [1:12:25<06:17,  9.69s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 446/484 [1:12:25<05:48,  9.16s/it, training_loss=0.219]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 446/484 [1:12:36<05:48,  9.16s/it, training_loss=0.164]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 447/484 [1:12:36<05:59,  9.72s/it, training_loss=0.164]\u001b[A\n",
            "Epoch 1:  92%|█████████▏| 447/484 [1:12:44<05:59,  9.72s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 448/484 [1:12:44<05:30,  9.17s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 448/484 [1:12:55<05:30,  9.17s/it, training_loss=0.238]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 449/484 [1:12:55<05:39,  9.71s/it, training_loss=0.238]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 449/484 [1:13:03<05:39,  9.71s/it, training_loss=0.328]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 450/484 [1:13:03<05:10,  9.15s/it, training_loss=0.328]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 450/484 [1:13:14<05:10,  9.15s/it, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 451/484 [1:13:14<05:19,  9.68s/it, training_loss=0.244]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 451/484 [1:13:22<05:19,  9.68s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 452/484 [1:13:22<04:52,  9.14s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  93%|█████████▎| 452/484 [1:13:33<04:52,  9.14s/it, training_loss=0.215]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 453/484 [1:13:33<04:59,  9.67s/it, training_loss=0.215]\u001b[A\n",
            "Epoch 1:  94%|█████████▎| 453/484 [1:13:41<04:59,  9.67s/it, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 454/484 [1:13:41<04:34,  9.16s/it, training_loss=0.163]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 454/484 [1:13:52<04:34,  9.16s/it, training_loss=0.114]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 455/484 [1:13:52<04:39,  9.64s/it, training_loss=0.114]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 455/484 [1:14:00<04:39,  9.64s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 456/484 [1:14:00<04:18,  9.22s/it, training_loss=0.206]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 456/484 [1:14:10<04:18,  9.22s/it, training_loss=0.158]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 457/484 [1:14:10<04:18,  9.58s/it, training_loss=0.158]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 457/484 [1:14:23<04:18,  9.58s/it, training_loss=0.207]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 458/484 [1:14:23<04:34, 10.57s/it, training_loss=0.207]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 458/484 [1:14:32<04:34, 10.57s/it, training_loss=0.124]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 459/484 [1:14:32<04:12, 10.10s/it, training_loss=0.124]\u001b[A\n",
            "Epoch 1:  95%|█████████▍| 459/484 [1:14:42<04:12, 10.10s/it, training_loss=0.274]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 460/484 [1:14:42<04:01, 10.05s/it, training_loss=0.274]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 460/484 [1:14:51<04:01, 10.05s/it, training_loss=0.291]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 461/484 [1:14:51<03:41,  9.65s/it, training_loss=0.291]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 461/484 [1:15:01<03:41,  9.65s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 462/484 [1:15:01<03:35,  9.79s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  95%|█████████▌| 462/484 [1:15:09<03:35,  9.79s/it, training_loss=0.159]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 463/484 [1:15:10<03:17,  9.42s/it, training_loss=0.159]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 463/484 [1:15:20<03:17,  9.42s/it, training_loss=0.086]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 464/484 [1:15:20<03:13,  9.66s/it, training_loss=0.086]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 464/484 [1:15:28<03:13,  9.66s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 465/484 [1:15:28<02:56,  9.31s/it, training_loss=0.149]\u001b[A\n",
            "Epoch 1:  96%|█████████▌| 465/484 [1:15:39<02:56,  9.31s/it, training_loss=0.096]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 466/484 [1:15:39<02:54,  9.67s/it, training_loss=0.096]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 466/484 [1:15:47<02:54,  9.67s/it, training_loss=0.074]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 467/484 [1:15:47<02:37,  9.27s/it, training_loss=0.074]\u001b[A\n",
            "Epoch 1:  96%|█████████▋| 467/484 [1:15:58<02:37,  9.27s/it, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 468/484 [1:15:58<02:36,  9.79s/it, training_loss=0.285]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 468/484 [1:16:06<02:36,  9.79s/it, training_loss=0.442]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 469/484 [1:16:06<02:18,  9.24s/it, training_loss=0.442]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 469/484 [1:16:17<02:18,  9.24s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 470/484 [1:16:17<02:16,  9.77s/it, training_loss=0.146]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 470/484 [1:16:25<02:16,  9.77s/it, training_loss=0.133]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 471/484 [1:16:25<01:59,  9.19s/it, training_loss=0.133]\u001b[A\n",
            "Epoch 1:  97%|█████████▋| 471/484 [1:16:36<01:59,  9.19s/it, training_loss=0.110]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 472/484 [1:16:36<01:56,  9.73s/it, training_loss=0.110]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 472/484 [1:16:44<01:56,  9.73s/it, training_loss=0.098]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 473/484 [1:16:44<01:40,  9.17s/it, training_loss=0.098]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 473/484 [1:16:55<01:40,  9.17s/it, training_loss=0.225]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 474/484 [1:16:55<01:37,  9.71s/it, training_loss=0.225]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 474/484 [1:17:03<01:37,  9.71s/it, training_loss=0.097]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 475/484 [1:17:03<01:22,  9.16s/it, training_loss=0.097]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 475/484 [1:17:13<01:22,  9.16s/it, training_loss=0.374]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 476/484 [1:17:13<01:17,  9.69s/it, training_loss=0.374]\u001b[A\n",
            "Epoch 1:  98%|█████████▊| 476/484 [1:17:21<01:17,  9.69s/it, training_loss=0.089]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 477/484 [1:17:21<01:03,  9.14s/it, training_loss=0.089]\u001b[A\n",
            "Epoch 1:  99%|█████████▊| 477/484 [1:17:32<01:03,  9.14s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 478/484 [1:17:32<00:57,  9.66s/it, training_loss=0.154]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 478/484 [1:17:40<00:57,  9.66s/it, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 479/484 [1:17:40<00:45,  9.16s/it, training_loss=0.352]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 479/484 [1:17:51<00:45,  9.16s/it, training_loss=0.171]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 480/484 [1:17:51<00:38,  9.61s/it, training_loss=0.171]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 480/484 [1:17:59<00:38,  9.61s/it, training_loss=0.142]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 481/484 [1:17:59<00:27,  9.24s/it, training_loss=0.142]\u001b[A\n",
            "Epoch 1:  99%|█████████▉| 481/484 [1:18:09<00:27,  9.24s/it, training_loss=0.159]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 482/484 [1:18:09<00:19,  9.54s/it, training_loss=0.159]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 482/484 [1:18:18<00:19,  9.54s/it, training_loss=0.305]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 483/484 [1:18:18<00:09,  9.27s/it, training_loss=0.305]\u001b[A\n",
            "Epoch 1: 100%|█████████▉| 483/484 [1:18:26<00:09,  9.27s/it, training_loss=0.212]\u001b[A\n",
            "Epoch 1: 100%|██████████| 484/484 [1:18:26<00:00,  8.97s/it, training_loss=0.212]\u001b[A\n",
            "  0%|          | 0/1 [1:18:27<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-f3696705aeee>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'training_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'{:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'Models2/ BERT_ft_epoch{epoch}.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n Epoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory Models2 does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f'/content/gdrive/MyDrive/NLP/BERT_ft_epoch{epoch}.model')\n"
      ],
      "metadata": {
        "id": "czvfR_xKiPYv"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.write('\\n Epoch {epoch}')\n",
        "    \n",
        "loss_train_ave = loss_train_total / len(dataloader_train)\n",
        "tqdm.write('Training loss: {loss_train_avg}')\n",
        "    \n",
        "val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
        "val_f1 = f1_score_func(predictions, true_vals)\n",
        "tqdm.write(f'Validation loss: {val_loss}')\n",
        "tqdm.write(f'F1 Score (weighted): {val_f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRf5q7y2ifjV",
        "outputId": "75edede6-e157-4978-cb54-a51f36d45fc5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch {epoch}\n",
            "Training loss: {loss_train_avg}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 121/121 [06:15<00:00,  3.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.4506883809635462\n",
            "F1 Score (weighted): 0.8401579932844422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVQkfAZWiflx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, predictions, true_vals = evaluate(dataloader_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPQEu0MvcrLJ",
        "outputId": "0ba48b3e-19d8-44f5-f766-452c6fd08d36"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 121/121 [06:39<00:00,  3.31s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict={'Negative':0,'Positive':1}\n",
        "true_vals.shape\n",
        "#predictions.shape\n",
        "accuracy_per_class(predictions, true_vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6l2MScTcro0",
        "outputId": "83d64b55-7396-464d-ce1e-142e77a40c76"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:102/120\n",
            " -> 0.85\n",
            "Accuracy:502/575\n",
            " -> 0.8730434782608696\n",
            "Accuracy:209/273\n",
            " -> 0.7655677655677655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = F\"/content/gdrive/MyDrive/NLP/BERT_ft_epoch1.model\"\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcm5wY4-i-Dy",
        "outputId": "2d747d45-0326-4c24-9ead-35d0f2cf909e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)"
      ],
      "metadata": {
        "id": "kozbV9TAi-GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Gulf Navigation Company announced that the Board of Directors of Gulf Navigation Holding Company (PJSC) held its meeting on April 21, 2011 and reviewed the financial results for the first quarter of 2011.\".apply(pipe)"
      ],
      "metadata": {
        "id": "v_TLGU_Ji-Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pyHwdyKi-LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HgSqUXVi-Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P18BJP2Mi-QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbtyjFj2i-Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhYrXL_ii-U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "taA6gD7-i-aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G42w38YXi-cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "id": "qp_4qBVmoC5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simpletransformers.classification import ClassificationModel\n",
        "\n",
        "\n",
        "# Create a TransformerModel\n",
        "model = ClassificationModel('bert', 'bert-base-cased', num_labels=3, args={'reprocess_input_data': True, 'overwrite_output_dir': True},use_cuda=False)"
      ],
      "metadata": {
        "id": "qnq13_yW_TX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0,1,2 : positive,negative\n",
        "def making_label(st):\n",
        "    if(st=='positive'):\n",
        "        return 0\n",
        "    elif(st=='neutral'):\n",
        "        return 2\n",
        "    else:\n",
        "        return 1\n",
        "    \n",
        "train['label'] = train['Sentiment'].apply(making_label)\n",
        "test['label'] = test['Sentiment'].apply(making_label)\n",
        "print(train.shape)"
      ],
      "metadata": {
        "id": "nS7bRBKb_WG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame({\n",
        "    'text': train['Sentence'][:1500].replace(r'\\n', ' ', regex=True),\n",
        "    'label': train['label'][:1500]\n",
        "})\n",
        "\n",
        "eval_df = pd.DataFrame({\n",
        "    'text': test['Sentence'][-400:].replace(r'\\n', ' ', regex=True),\n",
        "    'label': test['label'][-400:]\n",
        "})"
      ],
      "metadata": {
        "id": "jF-ruRta_4C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train_model(train_df)"
      ],
      "metadata": {
        "id": "mdvEoYNO_-cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
      ],
      "metadata": {
        "id": "f8eKnc76AEOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = []\n",
        "for arr in model_outputs:\n",
        "    lst.append(np.argmax(arr))"
      ],
      "metadata": {
        "id": "fOmt_agRzMyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true = eval_df['label'].tolist()\n",
        "predicted = lst"
      ],
      "metadata": {
        "id": "o5Er_aNnzaZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "sklearn.metrics.accuracy_score(true,predicted)"
      ],
      "metadata": {
        "id": "nojvATamza0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, raw_outputs = model.predict(\"Almost all the region's markets reversed in yesterday's session trading from the trend they were in the previous session in light of the dominance of buying operations in the markets throughout the trading period almost with the encouragement of some dealers to take financial positions on selected shares for the purpose of investment or speculation after the prices became more attractive and in Under the return of relative stability to the regional and global climate. Where only the Jordanian market declined under pressure from the services stocks, as it declined by 0.27%, to close its general index at the level of 2350.91 points. The Kuwaiti market bounced back to recover most of the losses of the previous session, supported by almost all sectors, amid a decline in liquidity, as it rose by 0.66%, to close at 6914.40 points. The Saudi stock market maintained its upward trend, with the support of the main sectors and stocks in the market, as it rose by 0.36%, to close at 43.6332 points. Buying forces returned to control the Qatari Stock Exchange to recoup most of the losses of the previous session, rising by 0.59% to close at 81158.87 points. With the rebound of heavy stocks in the Bahraini market from the levels of the previous session, the Bahraini market recovered all the losses, to close its index at 1438.87 points, with a gain of 0.83%. The Omani market continued its rise, supported by a number of its best stocks, as it rose by 0.31%, to close at 6602.95 points.\")"
      ],
      "metadata": {
        "id": "rIbPgdUW0XIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(predictions)"
      ],
      "metadata": {
        "id": "YGz_1jQX1Fa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading the MarianMTModel Translator"
      ],
      "metadata": {
        "id": "XGq5DTV-EMVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianTokenizer, MarianMTModel\n",
        "mname = \"marefa-nlp/marefa-mt-en-ar\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(mname)\n",
        "model = MarianMTModel.from_pretrained(mname)"
      ],
      "metadata": {
        "id": "kaVnG-iuyugI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translating the dataset"
      ],
      "metadata": {
        "id": "9jYWLnnxEo0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# English Text\n",
        "input = \"With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability.\"\n",
        "\n",
        "translated_tokens = model.generate(**tokenizer.prepare_seq2seq_batch([input], return_tensors=\"pt\"))\n",
        "translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
        "\n",
        "# translated Arabic Text\n",
        "print(translated_text)\n"
      ],
      "metadata": {
        "id": "aGfkclBC0wxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the text files to merge\n",
        "dir_path = 'gdrive/My Drive/Finance/'\n",
        "\n",
        "# List of all the text files in the directory\n",
        "files = os.listdir(dir_path)\n",
        "print(files)\n",
        "\n",
        "# Open the CSV file for writing\n",
        "with open('merged_file.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row to the CSV file\n",
        "    writer.writerow(['filename', 'content'])\n",
        "\n",
        "    # Iterate through all the text files in the directory\n",
        "    for file in files:\n",
        "        if file.endswith('.txt'):\n",
        "            # Open the text file for reading\n",
        "            with open(os.path.join(dir_path, file), 'r') as f:\n",
        "                content = f.read()\n",
        "                \n",
        "            # Write the filename and content to the CSV file\n",
        "            writer.writerow([file, content])\n",
        "\n",
        "# Close the CSV file\n",
        "csvfile.close()"
      ],
      "metadata": {
        "id": "OepVekXDBQ14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"merged_file.csv\")\n",
        "print(df.describe)"
      ],
      "metadata": {
        "id": "vG2Mz3dbDeis"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}